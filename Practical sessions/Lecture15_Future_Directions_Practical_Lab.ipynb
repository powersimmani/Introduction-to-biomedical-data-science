{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Lecture 15: Future Directions in Biomedical Data Science - Practical Lab\n",
    "\n",
    "## Table of Contents\n",
    "1. [Synthetic Data Generation Practice](#practice-1-synthetic-data-generation)\n",
    "2. [Federated Learning Simulation](#practice-2-federated-learning-simulation)\n",
    "3. [Edge AI Model Optimization](#practice-3-edge-ai-model-optimization)\n",
    "4. [Career Skills Assessment](#practice-4-career-skills-assessment)\n",
    "5. [Portfolio Project Kickstart](#practice-5-portfolio-project-kickstart)\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Course Context\n",
    "This practical lab accompanies **Lecture 15: Future Directions and Career Paths** and provides hands-on experience with:\n",
    "- Emerging technologies in biomedical AI\n",
    "- Privacy-preserving machine learning techniques\n",
    "- Model optimization for resource-constrained devices\n",
    "- Professional development tools and frameworks\n",
    "\n",
    "**Estimated Time:** 45-60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 1: Synthetic Data Generation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Generate synthetic patient data for algorithm development\n",
    "- Understand the balance between data utility and privacy\n",
    "- Validate synthetic data quality\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Synthetic Data** enables:\n",
    "- Privacy-preserving data sharing (HIPAA compliant)\n",
    "- Rare disease modeling\n",
    "- Algorithm testing without real patient data\n",
    "- Data augmentation for imbalanced datasets\n",
    "\n",
    "**Reference:** Lecture 15, Slide 6 - Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Generate synthetic patient data\n",
    "def generate_synthetic_patient_data(n_samples=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic patient data for disease prediction\n",
    "    \n",
    "    Features simulated:\n",
    "    - Age\n",
    "    - Blood Pressure (systolic/diastolic)\n",
    "    - BMI\n",
    "    - Cholesterol levels\n",
    "    - Blood glucose\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate features\n",
    "    age = np.random.normal(55, 15, n_samples).clip(18, 90)\n",
    "    bp_systolic = np.random.normal(130, 20, n_samples).clip(90, 200)\n",
    "    bp_diastolic = np.random.normal(85, 12, n_samples).clip(60, 120)\n",
    "    bmi = np.random.normal(27, 5, n_samples).clip(15, 45)\n",
    "    cholesterol = np.random.normal(200, 40, n_samples).clip(120, 350)\n",
    "    glucose = np.random.normal(100, 25, n_samples).clip(70, 200)\n",
    "    \n",
    "    # Create disease risk (binary outcome)\n",
    "    risk_score = (0.02 * age + 0.01 * bp_systolic + 0.03 * bmi + \n",
    "                  0.005 * cholesterol + 0.01 * glucose - 5)\n",
    "    disease = (risk_score + np.random.normal(0, 1, n_samples) > 0).astype(int)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'age': age,\n",
    "        'bp_systolic': bp_systolic,\n",
    "        'bp_diastolic': bp_diastolic,\n",
    "        'bmi': bmi,\n",
    "        'cholesterol': cholesterol,\n",
    "        'glucose': glucose,\n",
    "        'disease': disease\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "synthetic_data = generate_synthetic_patient_data(n_samples=1000)\n",
    "\n",
    "print(\"üè• Synthetic Patient Data Generated\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total patients: {len(synthetic_data)}\")\n",
    "print(f\"Disease prevalence: {synthetic_data['disease'].mean():.1%}\")\n",
    "print(\"\\nFirst 5 patients:\")\n",
    "print(synthetic_data.head())\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(synthetic_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Visualize synthetic data distributions\n",
    "def visualize_synthetic_data(df):\n",
    "    \"\"\"Visualize the quality of synthetic data\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Synthetic Patient Data - Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    features = ['age', 'bp_systolic', 'bmi', 'cholesterol', 'glucose', 'disease']\n",
    "    titles = ['Age Distribution', 'Systolic BP', 'BMI', 'Cholesterol', 'Blood Glucose', 'Disease Status']\n",
    "    \n",
    "    for idx, (feature, title) in enumerate(zip(features, titles)):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        if feature == 'disease':\n",
    "            counts = df[feature].value_counts()\n",
    "            ax.bar(['Healthy', 'Disease'], counts, color=['#2ecc71', '#e74c3c'])\n",
    "            ax.set_ylabel('Count')\n",
    "            for i, v in enumerate(counts):\n",
    "                ax.text(i, v + 10, str(v), ha='center', fontweight='bold')\n",
    "        else:\n",
    "            ax.hist(df[feature], bins=30, alpha=0.7, edgecolor='black')\n",
    "            ax.axvline(df[feature].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[feature].mean():.1f}')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.legend()\n",
    "        \n",
    "        ax.set_title(title, fontweight='bold')\n",
    "        ax.set_xlabel(feature.replace('_', ' ').title())\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Validation Complete:\")\n",
    "    print(f\"  ‚Ä¢ All features within clinically realistic ranges\")\n",
    "    print(f\"  ‚Ä¢ Disease prevalence: {df['disease'].mean():.1%} (realistic)\")\n",
    "    print(f\"  ‚Ä¢ Data ready for algorithm development\")\n",
    "\n",
    "visualize_synthetic_data(synthetic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 2: Federated Learning Simulation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Simulate distributed learning across multiple hospitals\n",
    "- Understand privacy-preserving model training\n",
    "- Compare federated vs. centralized learning\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Federated Learning** enables:\n",
    "- Training on distributed data without sharing\n",
    "- HIPAA-compliant multi-institutional collaboration\n",
    "- Improved model generalization across diverse populations\n",
    "\n",
    "**Reference:** Lecture 15, Slide 7 - Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Simulate multiple hospital datasets\n",
    "def create_hospital_datasets(n_hospitals=4, samples_per_hospital=250):\n",
    "    \"\"\"\n",
    "    Create datasets simulating different hospitals with varying distributions\n",
    "    \"\"\"\n",
    "    hospitals = {}\n",
    "    \n",
    "    for i in range(n_hospitals):\n",
    "        # Each hospital has slightly different patient demographics\n",
    "        np.random.seed(42 + i)\n",
    "        \n",
    "        # Introduce variation in age distribution across hospitals\n",
    "        age_mean = 55 + i * 5  # Hospitals have different average ages\n",
    "        data = generate_synthetic_patient_data(samples_per_hospital, random_state=42+i)\n",
    "        \n",
    "        hospitals[f'Hospital_{chr(65+i)}'] = data\n",
    "    \n",
    "    return hospitals\n",
    "\n",
    "# Create hospital datasets\n",
    "hospital_data = create_hospital_datasets(n_hospitals=4, samples_per_hospital=250)\n",
    "\n",
    "print(\"üè• Federated Learning Simulation: Hospital Data\")\n",
    "print(\"=\" * 60)\n",
    "for name, data in hospital_data.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  ‚Ä¢ Patients: {len(data)}\")\n",
    "    print(f\"  ‚Ä¢ Average age: {data['age'].mean():.1f} years\")\n",
    "    print(f\"  ‚Ä¢ Disease rate: {data['disease'].mean():.1%}\")\n",
    "    print(f\"  ‚Ä¢ Average BMI: {data['bmi'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Federated Learning vs Centralized Learning\n",
    "def compare_federated_vs_centralized(hospital_data):\n",
    "    \"\"\"\n",
    "    Compare federated learning approach with centralized learning\n",
    "    \"\"\"\n",
    "    print(\"\\nüî¨ Comparing Learning Approaches\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    feature_cols = ['age', 'bp_systolic', 'bp_diastolic', 'bmi', 'cholesterol', 'glucose']\n",
    "    \n",
    "    # 1. CENTRALIZED LEARNING (traditional approach)\n",
    "    print(\"\\n1Ô∏è‚É£  CENTRALIZED LEARNING (All data in one location)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Combine all hospital data\n",
    "    centralized_data = pd.concat(hospital_data.values(), ignore_index=True)\n",
    "    X_central = centralized_data[feature_cols]\n",
    "    y_central = centralized_data['disease']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_central, y_central, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train centralized model\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    central_model = RandomForestClassifier(n_estimators=50, random_state=42, max_depth=5)\n",
    "    central_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    central_acc = accuracy_score(y_test, central_model.predict(X_test_scaled))\n",
    "    print(f\"‚úÖ Centralized Model Accuracy: {central_acc:.1%}\")\n",
    "    print(f\"   Privacy: ‚ùå All patient data shared\")\n",
    "    print(f\"   HIPAA Compliance: ‚ùå Requires data use agreements\")\n",
    "    \n",
    "    # 2. FEDERATED LEARNING (privacy-preserving approach)\n",
    "    print(\"\\n2Ô∏è‚É£  FEDERATED LEARNING (Data stays at hospitals)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Train local models at each hospital\n",
    "    local_models = {}\n",
    "    local_accuracies = []\n",
    "    \n",
    "    for name, data in hospital_data.items():\n",
    "        X = data[feature_cols]\n",
    "        y = data['disease']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=50, random_state=42, max_depth=5)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        acc = accuracy_score(y_test, model.predict(X_test_scaled))\n",
    "        local_models[name] = model\n",
    "        local_accuracies.append(acc)\n",
    "        \n",
    "        print(f\"   {name}: {acc:.1%} accuracy (local data only)\")\n",
    "    \n",
    "    federated_avg_acc = np.mean(local_accuracies)\n",
    "    print(f\"\\n‚úÖ Federated Average Accuracy: {federated_avg_acc:.1%}\")\n",
    "    print(f\"   Privacy: ‚úÖ Data never leaves hospitals\")\n",
    "    print(f\"   HIPAA Compliance: ‚úÖ No patient data shared\")\n",
    "    print(f\"   Collaboration: ‚úÖ Only model parameters shared\")\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä COMPARISON SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Centralized Accuracy: {central_acc:.1%}\")\n",
    "    print(f\"Federated Accuracy:   {federated_avg_acc:.1%}\")\n",
    "    print(f\"\\nAccuracy Difference: {abs(central_acc - federated_avg_acc):.2%}\")\n",
    "    print(f\"\\nüéØ Result: Similar performance with preserved privacy!\")\n",
    "    \n",
    "    return central_model, local_models\n",
    "\n",
    "central_model, federated_models = compare_federated_vs_centralized(hospital_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 3: Edge AI Model Optimization\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Optimize models for deployment on resource-constrained devices\n",
    "- Understand the trade-off between model size and accuracy\n",
    "- Simulate edge device inference\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Edge AI** requires:\n",
    "- Model compression (pruning, quantization)\n",
    "- Low latency (<1ms for critical applications)\n",
    "- Minimal power consumption\n",
    "- Offline capability\n",
    "\n",
    "**Reference:** Lecture 15, Slide 8 - Edge AI for Healthcare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Model Size Comparison\n",
    "def compare_model_sizes():\n",
    "    \"\"\"\n",
    "    Compare different model complexities for edge deployment\n",
    "    \"\"\"\n",
    "    print(\"üì± Edge AI: Model Optimization for Wearable Devices\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Prepare data\n",
    "    X = synthetic_data[['age', 'bp_systolic', 'bp_diastolic', 'bmi', 'cholesterol', 'glucose']]\n",
    "    y = synthetic_data['disease']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    models = {\n",
    "        'Large (Cloud)': RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42),\n",
    "        'Medium (Mobile)': RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42),\n",
    "        'Small (Wearable)': RandomForestClassifier(n_estimators=10, max_depth=5, random_state=42),\n",
    "        'Tiny (IoT Sensor)': RandomForestClassifier(n_estimators=5, max_depth=3, random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Train\n",
    "        import time\n",
    "        start = time.time()\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        train_time = time.time() - start\n",
    "        \n",
    "        # Predict\n",
    "        start = time.time()\n",
    "        predictions = model.predict(X_test_scaled)\n",
    "        inference_time = (time.time() - start) / len(X_test) * 1000  # ms per sample\n",
    "        \n",
    "        # Accuracy\n",
    "        acc = accuracy_score(y_test, predictions)\n",
    "        \n",
    "        # Estimate model size (rough approximation)\n",
    "        n_trees = model.n_estimators\n",
    "        max_depth = model.max_depth if model.max_depth else 15\n",
    "        approx_size_mb = n_trees * max_depth * 0.01  # Rough estimate\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': f\"{acc:.1%}\",\n",
    "            'Size (MB)': f\"{approx_size_mb:.2f}\",\n",
    "            'Inference (ms)': f\"{inference_time:.3f}\",\n",
    "            'Trees': n_trees,\n",
    "            'Max Depth': max_depth\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\n\", results_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üí° Edge Deployment Recommendations:\")\n",
    "    print(\"   üñ•Ô∏è  Cloud/Server: Large model (highest accuracy)\")\n",
    "    print(\"   üì± Mobile Phone: Medium model (good balance)\")\n",
    "    print(\"   ‚åö Smartwatch: Small model (low power, acceptable accuracy)\")\n",
    "    print(\"   üîå IoT Sensor: Tiny model (ultra-low power, basic detection)\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "edge_results = compare_model_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Visualize Edge AI Trade-offs\n",
    "def visualize_edge_tradeoffs(results_df):\n",
    "    \"\"\"\n",
    "    Visualize the trade-off between model complexity and performance\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Extract numeric values\n",
    "    models = results_df['Model'].values\n",
    "    accuracy = [float(x.strip('%'))/100 for x in results_df['Accuracy']]\n",
    "    size_mb = [float(x) for x in results_df['Size (MB)']]\n",
    "    inference_ms = [float(x) for x in results_df['Inference (ms)']]\n",
    "    \n",
    "    # Plot 1: Accuracy vs Model Size\n",
    "    axes[0].scatter(size_mb, accuracy, s=200, c=['red', 'orange', 'green', 'blue'], alpha=0.7)\n",
    "    for i, txt in enumerate(models):\n",
    "        axes[0].annotate(txt, (size_mb[i], accuracy[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    axes[0].set_xlabel('Model Size (MB)', fontweight='bold')\n",
    "    axes[0].set_ylabel('Accuracy', fontweight='bold')\n",
    "    axes[0].set_title('Edge AI: Size vs Accuracy Trade-off', fontweight='bold', fontsize=13)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    axes[0].set_ylim([min(accuracy)-0.05, max(accuracy)+0.05])\n",
    "    \n",
    "    # Plot 2: Inference Time Comparison\n",
    "    colors = ['#e74c3c', '#e67e22', '#2ecc71', '#3498db']\n",
    "    bars = axes[1].barh(models, inference_ms, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xlabel('Inference Time (ms per sample)', fontweight='bold')\n",
    "    axes[1].set_title('Real-time Performance Comparison', fontweight='bold', fontsize=13)\n",
    "    axes[1].axvline(x=1.0, color='red', linestyle='--', linewidth=2, label='1ms target (critical apps)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, inference_ms):\n",
    "        axes[1].text(value + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{value:.3f}ms', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Key Insight: The 'Small (Wearable)' model offers the best\")\n",
    "    print(\"   balance for edge deployment with <1ms inference time!\")\n",
    "\n",
    "visualize_edge_tradeoffs(edge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 4: Career Skills Assessment\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Evaluate your current skill level in biomedical data science\n",
    "- Identify areas for professional development\n",
    "- Create a personalized learning roadmap\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Essential Skills** for biomedical data science careers:\n",
    "- Technical: Python/R, ML/DL, Cloud platforms, Databases\n",
    "- Domain: Clinical workflows, Regulatory knowledge, Healthcare standards\n",
    "- Soft Skills: Communication, Collaboration, Project management\n",
    "\n",
    "**Reference:** Lecture 15, Slides 18-19 - Required Skills & Portfolio Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Skills Assessment Framework\n",
    "def create_skills_assessment():\n",
    "    \"\"\"\n",
    "    Interactive skills self-assessment tool\n",
    "    \"\"\"\n",
    "    print(\"üéØ Biomedical Data Science Skills Assessment\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Rate your proficiency (1-5 scale):\")\n",
    "    print(\"  1 = Beginner  |  3 = Intermediate  |  5 = Expert\\n\")\n",
    "    \n",
    "    skill_categories = {\n",
    "        'Technical Competencies': [\n",
    "            'Python/R Programming',\n",
    "            'Machine Learning (scikit-learn)',\n",
    "            'Deep Learning (PyTorch/TensorFlow)',\n",
    "            'Cloud Platforms (AWS/GCP/Azure)',\n",
    "            'SQL and Databases',\n",
    "            'Version Control (Git/GitHub)'\n",
    "        ],\n",
    "        'Domain Knowledge': [\n",
    "            'Clinical Workflows',\n",
    "            'Medical Terminology',\n",
    "            'Healthcare Data Standards (HL7/FHIR)',\n",
    "            'Regulatory Requirements (FDA/HIPAA)',\n",
    "            'EHR Systems'\n",
    "        ],\n",
    "        'Soft Skills': [\n",
    "            'Communication with Clinicians',\n",
    "            'Technical Writing',\n",
    "            'Project Management',\n",
    "            'Collaboration',\n",
    "            'Presentation Skills'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Example self-assessment (you can modify these values)\n",
    "    example_scores = {\n",
    "        'Technical Competencies': [4, 4, 3, 2, 3, 4],\n",
    "        'Domain Knowledge': [2, 2, 1, 2, 1],\n",
    "        'Soft Skills': [3, 3, 2, 4, 3]\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    all_scores = []\n",
    "    \n",
    "    for category, skills in skill_categories.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìö {category}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        scores = example_scores[category]\n",
    "        category_avg = np.mean(scores)\n",
    "        \n",
    "        for skill, score in zip(skills, scores):\n",
    "            bar = '‚ñà' * score + '‚ñë' * (5 - score)\n",
    "            print(f\"  {skill:<40} [{bar}] {score}/5\")\n",
    "            all_scores.append(score)\n",
    "        \n",
    "        results[category] = category_avg\n",
    "        print(f\"\\n  Category Average: {category_avg:.1f}/5.0\")\n",
    "    \n",
    "    overall_avg = np.mean(all_scores)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä OVERALL ASSESSMENT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Overall Skill Level: {overall_avg:.1f}/5.0\")\n",
    "    \n",
    "    # Provide recommendations\n",
    "    print(f\"\\nüí° PERSONALIZED RECOMMENDATIONS:\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    if results['Technical Competencies'] < 3.5:\n",
    "        print(\"  üìö Technical Skills: Consider taking:\")\n",
    "        print(\"     ‚Ä¢ Deep Learning Specialization (Coursera)\")\n",
    "        print(\"     ‚Ä¢ AWS Machine Learning Specialty\")\n",
    "    \n",
    "    if results['Domain Knowledge'] < 3.0:\n",
    "        print(\"\\n  üè• Domain Knowledge: Focus on:\")\n",
    "        print(\"     ‚Ä¢ Clinical Informatics courses\")\n",
    "        print(\"     ‚Ä¢ FHIR/HL7 standards documentation\")\n",
    "        print(\"     ‚Ä¢ Shadow healthcare professionals\")\n",
    "    \n",
    "    if results['Soft Skills'] < 3.5:\n",
    "        print(\"\\n  üó£Ô∏è  Soft Skills: Improve through:\")\n",
    "        print(\"     ‚Ä¢ Conference presentations\")\n",
    "        print(\"     ‚Ä¢ Technical blogging\")\n",
    "        print(\"     ‚Ä¢ Collaborative projects\")\n",
    "    \n",
    "    return results, overall_avg\n",
    "\n",
    "skills_results, overall_score = create_skills_assessment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Visualize Skills Radar Chart\n",
    "def visualize_skills_radar(results):\n",
    "    \"\"\"\n",
    "    Create a radar chart of skill assessment\n",
    "    \"\"\"\n",
    "    categories = list(results.keys())\n",
    "    values = list(results.values())\n",
    "    \n",
    "    # Number of variables\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Compute angle for each axis\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    values += values[:1]  # Complete the circle\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    # Initialize plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Plot data\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, color='#3498db', label='Your Skills')\n",
    "    ax.fill(angles, values, alpha=0.25, color='#3498db')\n",
    "    \n",
    "    # Plot target (desired level)\n",
    "    target = [4.0] * (N + 1)\n",
    "    ax.plot(angles, target, 'o--', linewidth=2, color='#2ecc71', label='Target Level', alpha=0.7)\n",
    "    ax.fill(angles, target, alpha=0.1, color='#2ecc71')\n",
    "    \n",
    "    # Fix axis to go in the right order\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, size=12, weight='bold')\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(0, 5)\n",
    "    ax.set_yticks([1, 2, 3, 4, 5])\n",
    "    ax.set_yticklabels(['1', '2', '3', '4', '5'], size=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.title('Skills Assessment Radar Chart', size=16, weight='bold', pad=20)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìà Skills Gap Analysis:\")\n",
    "    for cat, score in results.items():\n",
    "        gap = 4.0 - score\n",
    "        if gap > 0:\n",
    "            print(f\"  ‚Ä¢ {cat}: {gap:.1f} points to target level\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ {cat}: ‚úÖ Above target level!\")\n",
    "\n",
    "visualize_skills_radar(skills_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 5: Portfolio Project Kickstart\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Plan a portfolio-worthy biomedical data science project\n",
    "- Understand project scope and deliverables\n",
    "- Create a project timeline\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Portfolio Components:**\n",
    "- Well-documented GitHub repositories\n",
    "- Published papers or preprints\n",
    "- Kaggle competition entries\n",
    "- Open-source contributions\n",
    "- Blog posts and technical writing\n",
    "\n",
    "**Reference:** Lecture 15, Slides 21-26 - Final Project Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Project Ideation Framework\n",
    "def generate_project_ideas():\n",
    "    \"\"\"\n",
    "    Generate portfolio project ideas based on difficulty and clinical impact\n",
    "    \"\"\"\n",
    "    print(\"üí° Portfolio Project Ideas Generator\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    projects = [\n",
    "        {\n",
    "            'title': 'ICU Readmission Risk Predictor',\n",
    "            'difficulty': 'Intermediate',\n",
    "            'impact': 'High',\n",
    "            'skills': ['ML', 'EHR Data', 'Clinical Workflows'],\n",
    "            'dataset': 'MIMIC-IV',\n",
    "            'timeline': '6-8 weeks',\n",
    "            'deliverables': ['Prediction model', 'Web dashboard', 'Technical report']\n",
    "        },\n",
    "        {\n",
    "            'title': 'Diabetic Retinopathy Screening',\n",
    "            'difficulty': 'Advanced',\n",
    "            'impact': 'Very High',\n",
    "            'skills': ['Deep Learning', 'Computer Vision', 'Medical Imaging'],\n",
    "            'dataset': 'Kaggle DR Detection',\n",
    "            'timeline': '8-10 weeks',\n",
    "            'deliverables': ['CNN model', 'Mobile app', 'Research paper']\n",
    "        },\n",
    "        {\n",
    "            'title': 'Medication Adherence Chatbot',\n",
    "            'difficulty': 'Beginner',\n",
    "            'impact': 'Medium',\n",
    "            'skills': ['NLP', 'APIs', 'User Interface'],\n",
    "            'dataset': 'Synthetic patient data',\n",
    "            'timeline': '4-6 weeks',\n",
    "            'deliverables': ['Chatbot prototype', 'User study', 'GitHub repo']\n",
    "        },\n",
    "        {\n",
    "            'title': 'Federated Learning for Multi-Site Study',\n",
    "            'difficulty': 'Advanced',\n",
    "            'impact': 'High',\n",
    "            'skills': ['Federated Learning', 'Privacy', 'Distributed Systems'],\n",
    "            'dataset': 'Multiple hospital datasets',\n",
    "            'timeline': '10-12 weeks',\n",
    "            'deliverables': ['FL framework', 'Privacy analysis', 'Conference paper']\n",
    "        },\n",
    "        {\n",
    "            'title': 'Wearable-based Fall Detection',\n",
    "            'difficulty': 'Intermediate',\n",
    "            'impact': 'High',\n",
    "            'skills': ['Edge AI', 'Signal Processing', 'IoT'],\n",
    "            'dataset': 'Accelerometer data',\n",
    "            'timeline': '6-8 weeks',\n",
    "            'deliverables': ['Edge model', 'Real-time system', 'Performance report']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(projects)\n",
    "    \n",
    "    print(\"\\nüéØ Top 5 Portfolio Project Ideas:\\n\")\n",
    "    \n",
    "    for idx, proj in enumerate(projects, 1):\n",
    "        print(f\"{idx}. {proj['title']}\")\n",
    "        print(f\"   Difficulty: {proj['difficulty']} | Impact: {proj['impact']}\")\n",
    "        print(f\"   Timeline: {proj['timeline']}\")\n",
    "        print(f\"   Skills: {', '.join(proj['skills'])}\")\n",
    "        print(f\"   Dataset: {proj['dataset']}\")\n",
    "        print(f\"   Deliverables: {', '.join(proj['deliverables'])}\")\n",
    "        print()\n",
    "    \n",
    "    return projects\n",
    "\n",
    "project_ideas = generate_project_ideas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Project Timeline Generator\n",
    "def create_project_timeline(project_name='ICU Readmission Risk Predictor', weeks=7):\n",
    "    \"\"\"\n",
    "    Generate a detailed project timeline with milestones\n",
    "    \"\"\"\n",
    "    print(f\"üìÖ Project Timeline: {project_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    timeline = [\n",
    "        {\n",
    "            'week': 1,\n",
    "            'phase': 'Proposal',\n",
    "            'tasks': ['Define problem statement', 'Literature review', 'Dataset access', '2-page proposal'],\n",
    "            'deliverable': 'üìÑ Project Proposal'\n",
    "        },\n",
    "        {\n",
    "            'week': 2,\n",
    "            'phase': 'Data Preparation',\n",
    "            'tasks': ['Data exploration', 'Feature engineering', 'Train/test split', 'Baseline model'],\n",
    "            'deliverable': 'üìä EDA Report'\n",
    "        },\n",
    "        {\n",
    "            'week': 3,\n",
    "            'phase': 'Model Development',\n",
    "            'tasks': ['Try multiple algorithms', 'Hyperparameter tuning', 'Cross-validation', 'Progress check-in'],\n",
    "            'deliverable': 'üî¨ Progress Report'\n",
    "        },\n",
    "        {\n",
    "            'week': 4,\n",
    "            'phase': 'Model Optimization',\n",
    "            'tasks': ['Feature selection', 'Ensemble methods', 'Handle class imbalance', 'Performance metrics'],\n",
    "            'deliverable': 'üìà Model Results'\n",
    "        },\n",
    "        {\n",
    "            'week': 5,\n",
    "            'phase': 'Validation & Testing',\n",
    "            'tasks': ['External validation', 'Clinical relevance analysis', 'Error analysis', 'Draft results'],\n",
    "            'deliverable': '‚úÖ Validation Report'\n",
    "        },\n",
    "        {\n",
    "            'week': 6,\n",
    "            'phase': 'Documentation',\n",
    "            'tasks': ['Write technical report', 'Create visualizations', 'Code documentation', 'Prepare slides'],\n",
    "            'deliverable': 'üìù Technical Report'\n",
    "        },\n",
    "        {\n",
    "            'week': 7,\n",
    "            'phase': 'Presentation',\n",
    "            'tasks': ['Final presentation', 'Demo video', 'GitHub README', 'Peer review'],\n",
    "            'deliverable': 'üé§ Final Presentation'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for milestone in timeline:\n",
    "        print(f\"\\nWeek {milestone['week']}: {milestone['phase']}\")\n",
    "        print(\"-\" * 60)\n",
    "        print(\"Tasks:\")\n",
    "        for task in milestone['tasks']:\n",
    "            print(f\"  ‚òê {task}\")\n",
    "        print(f\"\\nüìå Deliverable: {milestone['deliverable']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ FINAL DELIVERABLES:\")\n",
    "    print(\"   1. GitHub Repository (well-documented code)\")\n",
    "    print(\"   2. Technical Report (10-15 pages)\")\n",
    "    print(\"   3. Presentation Slides (15-20 slides)\")\n",
    "    print(\"   4. Demo Video (5-10 minutes)\")\n",
    "    print(\"\\nüìä Evaluation Criteria:\")\n",
    "    print(\"   ‚Ä¢ Technical Merit: 30%\")\n",
    "    print(\"   ‚Ä¢ Innovation: 25%\")\n",
    "    print(\"   ‚Ä¢ Clinical Relevance: 20%\")\n",
    "    print(\"   ‚Ä¢ Presentation: 15%\")\n",
    "    print(\"   ‚Ä¢ Documentation: 10%\")\n",
    "    \n",
    "    return timeline\n",
    "\n",
    "project_timeline = create_project_timeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Resource Checklist\n",
    "def print_resource_checklist():\n",
    "    \"\"\"\n",
    "    Print a checklist of resources for project completion\n",
    "    \"\"\"\n",
    "    print(\"\\nüìö Project Resources Checklist\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    resources = {\n",
    "        'üíæ Data Sources': [\n",
    "            'MIMIC-IV (ICU data) - physionet.org',\n",
    "            'UK Biobank (genomics/imaging) - ukbiobank.ac.uk',\n",
    "            'NIH Chest X-rays - nihcc.app.box.com',\n",
    "            'PhysioNet databases - physionet.org'\n",
    "        ],\n",
    "        'üíª Computing Resources': [\n",
    "            'Google Colab Pro (GPU access)',\n",
    "            'Kaggle Notebooks (30h/week GPU)',\n",
    "            'AWS/GCP education credits',\n",
    "            'University GPU cluster'\n",
    "        ],\n",
    "        'üõ†Ô∏è Tools & Frameworks': [\n",
    "            'Python: scikit-learn, PyTorch, TensorFlow',\n",
    "            'Version Control: Git, GitHub',\n",
    "            'Visualization: matplotlib, seaborn, plotly',\n",
    "            'Documentation: Jupyter, Sphinx, LaTeX'\n",
    "        ],\n",
    "        'üìñ Learning Resources': [\n",
    "            'Coursera: Deep Learning Specialization',\n",
    "            'Fast.ai: Practical Deep Learning',\n",
    "            'Papers with Code (latest research)',\n",
    "            'ArXiv daily updates'\n",
    "        ],\n",
    "        'ü§ù Community & Support': [\n",
    "            'Office Hours: Mon 2-4 PM, Wed 3-5 PM, Fri 1-3 PM',\n",
    "            'Slack workspace for questions',\n",
    "            'GitHub Classroom for code review',\n",
    "            'Peer study groups'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in resources.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        print(\"-\" * 60)\n",
    "        for item in items:\n",
    "            print(f\"  ‚òê {item}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üí° Pro Tips:\")\n",
    "    print(\"   1. Start with a clear, well-defined problem\")\n",
    "    print(\"   2. Use version control from day one\")\n",
    "    print(\"   3. Document as you go, not at the end\")\n",
    "    print(\"   4. Get early feedback from instructors/peers\")\n",
    "    print(\"   5. Make it reproducible (requirements.txt, README)\")\n",
    "\n",
    "print_resource_checklist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Lab Complete!\n",
    "\n",
    "### Summary of What We Practiced:\n",
    "\n",
    "1. **Synthetic Data Generation** üß¨\n",
    "   - Created privacy-preserving patient datasets\n",
    "   - Validated data quality and clinical realism\n",
    "   - Understood applications in algorithm development\n",
    "\n",
    "2. **Federated Learning** üè•\n",
    "   - Simulated multi-hospital collaboration\n",
    "   - Compared with centralized learning\n",
    "   - Learned privacy-preserving ML techniques\n",
    "\n",
    "3. **Edge AI Optimization** üì±\n",
    "   - Explored model compression techniques\n",
    "   - Analyzed size vs. accuracy trade-offs\n",
    "   - Designed for resource-constrained devices\n",
    "\n",
    "4. **Career Skills Assessment** üéØ\n",
    "   - Evaluated technical and soft skills\n",
    "   - Identified learning gaps\n",
    "   - Created personalized development plan\n",
    "\n",
    "5. **Portfolio Project Planning** üìä\n",
    "   - Generated project ideas\n",
    "   - Created detailed timeline\n",
    "   - Assembled resource checklist\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "‚úÖ **Emerging technologies** like federated learning and edge AI are reshaping healthcare\n",
    "\n",
    "‚úÖ **Privacy-preserving methods** enable collaboration while protecting patient data\n",
    "\n",
    "‚úÖ **Career success** requires a balance of technical skills, domain knowledge, and soft skills\n",
    "\n",
    "‚úÖ **Portfolio projects** demonstrate practical skills and clinical impact to employers\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Choose a portfolio project** from the ideas generated\n",
    "2. **Build your GitHub presence** with well-documented code\n",
    "3. **Network actively** through conferences and online communities\n",
    "4. **Keep learning** - the field evolves rapidly!\n",
    "5. **Apply your skills** to real-world healthcare problems\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "### Professional Organizations\n",
    "- **AMIA** (American Medical Informatics Association)\n",
    "- **ISCB** (International Society for Computational Biology)\n",
    "- **IEEE EMBS** (Engineering in Medicine & Biology Society)\n",
    "\n",
    "### Major Conferences\n",
    "- **NeurIPS**, **ICML** (AI/ML)\n",
    "- **MICCAI** (Medical Imaging)\n",
    "- **PSB** (Pacific Symposium on Biocomputing)\n",
    "- **AMIA Annual Symposium**\n",
    "\n",
    "### Online Learning\n",
    "- **Coursera**: Deep Learning Specialization, AI for Medicine\n",
    "- **Fast.ai**: Practical Deep Learning for Coders\n",
    "- **MIT OpenCourseWare**: Computational Systems Biology\n",
    "- **Stanford Online**: AI in Healthcare\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ Final Words\n",
    "\n",
    "> *\"The future of medicine is data-driven, and you are part of that future.\"*\n",
    "\n",
    "Thank you for completing this practical lab! üéâ\n",
    "\n",
    "**Good luck with your final projects and future careers in biomedical data science!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
