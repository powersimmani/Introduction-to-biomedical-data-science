{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß¨ NGS Data Analysis: Hands-on Practice\n",
    "\n",
    "**Lecture 4: Next-Generation Sequencing and Genomics**\n",
    "\n",
    "Instructor: Ho-min Park  \n",
    "Email: homin.park@ghent.ac.kr | powersimmani@gmail.com\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [FASTQ File Format and Quality Scores](#practice-1-fastq-file-format-and-quality-scores)\n",
    "2. [Quality Control with FastQC](#practice-2-quality-control-with-fastqc)\n",
    "3. [Read Alignment and SAM/BAM Format](#practice-3-read-alignment-and-sambam-format)\n",
    "4. [Variant Calling and VCF Format](#practice-4-variant-calling-and-vcf-format)\n",
    "5. [Variant Annotation and Interpretation](#practice-5-variant-annotation-and-interpretation)\n",
    "6. [RNA-seq Data Analysis](#practice-6-rna-seq-data-analysis)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required bioinformatics libraries (run once)\n",
    "# !pip install biopython pysam pandas matplotlib seaborn numpy\n",
    "\n",
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 1: FASTQ File Format and Quality Scores\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand FASTQ file structure\n",
    "- Convert Phred quality scores to probability\n",
    "- Analyze base quality distribution\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**FASTQ Format Structure:**\n",
    "```\n",
    "@SEQ_ID (Sequence identifier)\n",
    "GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n",
    "+ (Separator)\n",
    "!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65\n",
    "```\n",
    "\n",
    "**Phred Quality Score:** Q = -10 √ó log‚ÇÅ‚ÇÄ(P)  \n",
    "- Q30 = 99.9% accuracy (1 error in 1,000 bases)\n",
    "- Q40 = 99.99% accuracy (1 error in 10,000 bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Create simulated FASTQ data\n",
    "def create_simulated_fastq():\n",
    "    \"\"\"Generate simulated FASTQ reads for practice\"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simulate 5 reads\n",
    "    fastq_data = []\n",
    "    bases = ['A', 'T', 'G', 'C']\n",
    "    \n",
    "    for i in range(5):\n",
    "        # Generate random sequence\n",
    "        seq_length = 60\n",
    "        sequence = ''.join(np.random.choice(bases, seq_length))\n",
    "        \n",
    "        # Generate quality scores (ASCII 33-73 = Phred 0-40)\n",
    "        # Higher quality at the start, lower at the end (typical for Illumina)\n",
    "        quality_scores = []\n",
    "        for pos in range(seq_length):\n",
    "            if pos < 20:\n",
    "                q = np.random.randint(35, 41)  # High quality\n",
    "            elif pos < 40:\n",
    "                q = np.random.randint(30, 36)  # Medium quality\n",
    "            else:\n",
    "                q = np.random.randint(20, 31)  # Lower quality\n",
    "            quality_scores.append(chr(q + 33))\n",
    "        \n",
    "        quality_string = ''.join(quality_scores)\n",
    "        \n",
    "        # FASTQ entry\n",
    "        fastq_entry = {\n",
    "            'id': f'@READ_{i+1}',\n",
    "            'sequence': sequence,\n",
    "            'separator': '+',\n",
    "            'quality': quality_string\n",
    "        }\n",
    "        fastq_data.append(fastq_entry)\n",
    "    \n",
    "    return fastq_data\n",
    "\n",
    "# Generate data\n",
    "fastq_reads = create_simulated_fastq()\n",
    "\n",
    "# Display first read\n",
    "print(\"Example FASTQ Entry:\")\n",
    "print(\"=\" * 70)\n",
    "read = fastq_reads[0]\n",
    "print(read['id'])\n",
    "print(read['sequence'])\n",
    "print(read['separator'])\n",
    "print(read['quality'])\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Convert quality scores to Phred scores and error probability\n",
    "def analyze_quality_scores(quality_string):\n",
    "    \"\"\"Convert ASCII quality to Phred scores and error probabilities\"\"\"\n",
    "    \n",
    "    phred_scores = [ord(char) - 33 for char in quality_string]\n",
    "    error_probs = [10 ** (-q / 10) for q in phred_scores]\n",
    "    accuracy = [(1 - p) * 100 for p in error_probs]\n",
    "    \n",
    "    return phred_scores, error_probs, accuracy\n",
    "\n",
    "# Analyze first read\n",
    "quality_str = fastq_reads[0]['quality']\n",
    "phred, errors, acc = analyze_quality_scores(quality_str)\n",
    "\n",
    "# Create results DataFrame\n",
    "df_quality = pd.DataFrame({\n",
    "    'Position': range(1, len(quality_str) + 1),\n",
    "    'ASCII_Char': list(quality_str),\n",
    "    'Phred_Score': phred,\n",
    "    'Error_Probability': errors,\n",
    "    'Accuracy_%': acc\n",
    "})\n",
    "\n",
    "print(\"\\nQuality Score Analysis (First 10 positions):\")\n",
    "print(df_quality.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nüìä Summary Statistics:\")\n",
    "print(f\"Mean Phred Score: {np.mean(phred):.2f}\")\n",
    "print(f\"Minimum Phred Score: {np.min(phred)}\")\n",
    "print(f\"Maximum Phred Score: {np.max(phred)}\")\n",
    "print(f\"% of bases with Q ‚â• 30: {(np.array(phred) >= 30).sum() / len(phred) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Visualize quality scores across read positions\n",
    "def plot_quality_distribution(fastq_data):\n",
    "    \"\"\"Plot quality score distribution across all reads\"\"\"\n",
    "    \n",
    "    # Collect quality scores for each position\n",
    "    all_qualities = []\n",
    "    for read in fastq_data:\n",
    "        phred, _, _ = analyze_quality_scores(read['quality'])\n",
    "        all_qualities.append(phred)\n",
    "    \n",
    "    all_qualities = np.array(all_qualities)\n",
    "    \n",
    "    # Calculate statistics per position\n",
    "    mean_q = np.mean(all_qualities, axis=0)\n",
    "    q25 = np.percentile(all_qualities, 25, axis=0)\n",
    "    q75 = np.percentile(all_qualities, 75, axis=0)\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Quality across positions\n",
    "    positions = range(1, len(mean_q) + 1)\n",
    "    ax1.plot(positions, mean_q, 'b-', linewidth=2, label='Mean Quality')\n",
    "    ax1.fill_between(positions, q25, q75, alpha=0.3, color='blue', label='25-75 percentile')\n",
    "    ax1.axhline(y=30, color='green', linestyle='--', label='Q30 threshold')\n",
    "    ax1.axhline(y=20, color='orange', linestyle='--', label='Q20 threshold')\n",
    "    ax1.set_xlabel('Position in read (bp)')\n",
    "    ax1.set_ylabel('Phred Quality Score')\n",
    "    ax1.set_title('Per Base Sequence Quality')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Quality score distribution (histogram)\n",
    "    all_scores_flat = all_qualities.flatten()\n",
    "    ax2.hist(all_scores_flat, bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    ax2.axvline(x=30, color='green', linestyle='--', linewidth=2, label='Q30')\n",
    "    ax2.set_xlabel('Phred Quality Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Overall Quality Score Distribution')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Quality visualization complete!\")\n",
    "    print(f\"Average quality score: {np.mean(all_scores_flat):.2f}\")\n",
    "\n",
    "plot_quality_distribution(fastq_reads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 2: Quality Control with FastQC\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand key FastQC metrics\n",
    "- Identify common quality issues\n",
    "- Decide on filtering thresholds\n",
    "\n",
    "### üìñ Key Metrics\n",
    "- **Per base sequence quality**: Quality drops at read ends\n",
    "- **Per sequence quality scores**: Overall read quality distribution\n",
    "- **Sequence duplication levels**: PCR duplicates\n",
    "- **Adapter content**: Leftover adapter sequences\n",
    "- **GC content**: Should match expected distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Analyze GC content\n",
    "def calculate_gc_content(sequence):\n",
    "    \"\"\"Calculate GC percentage of a sequence\"\"\"\n",
    "    gc_count = sequence.count('G') + sequence.count('C')\n",
    "    return (gc_count / len(sequence)) * 100\n",
    "\n",
    "def analyze_gc_distribution(fastq_data):\n",
    "    \"\"\"Analyze GC content distribution across reads\"\"\"\n",
    "    \n",
    "    gc_contents = []\n",
    "    for read in fastq_data:\n",
    "        gc = calculate_gc_content(read['sequence'])\n",
    "        gc_contents.append(gc)\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(gc_contents, bins=15, color='teal', edgecolor='black', alpha=0.7)\n",
    "    plt.axvline(x=50, color='red', linestyle='--', linewidth=2, label='Expected 50%')\n",
    "    plt.axvline(x=np.mean(gc_contents), color='orange', linestyle='-', linewidth=2, \n",
    "                label=f'Mean: {np.mean(gc_contents):.1f}%')\n",
    "    plt.xlabel('GC Content (%)')\n",
    "    plt.ylabel('Number of Reads')\n",
    "    plt.title('GC Content Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Mean GC content: {np.mean(gc_contents):.2f}%\")\n",
    "    print(f\"Std GC content: {np.std(gc_contents):.2f}%\")\n",
    "    print(f\"Range: {np.min(gc_contents):.2f}% - {np.max(gc_contents):.2f}%\")\n",
    "    \n",
    "    return gc_contents\n",
    "\n",
    "gc_dist = analyze_gc_distribution(fastq_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Detect sequence duplicates\n",
    "def detect_duplicates(fastq_data):\n",
    "    \"\"\"Identify duplicate sequences\"\"\"\n",
    "    \n",
    "    sequence_counts = defaultdict(int)\n",
    "    for read in fastq_data:\n",
    "        sequence_counts[read['sequence']] += 1\n",
    "    \n",
    "    # Calculate duplication statistics\n",
    "    total_reads = len(fastq_data)\n",
    "    unique_reads = len(sequence_counts)\n",
    "    duplicated_reads = sum(1 for count in sequence_counts.values() if count > 1)\n",
    "    \n",
    "    duplication_rate = (total_reads - unique_reads) / total_reads * 100\n",
    "    \n",
    "    print(\"Sequence Duplication Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total reads: {total_reads}\")\n",
    "    print(f\"Unique sequences: {unique_reads}\")\n",
    "    print(f\"Duplicated sequences: {duplicated_reads}\")\n",
    "    print(f\"Duplication rate: {duplication_rate:.2f}%\")\n",
    "    \n",
    "    # Show most duplicated sequences\n",
    "    if duplicated_reads > 0:\n",
    "        print(\"\\nMost duplicated sequences:\")\n",
    "        sorted_seqs = sorted(sequence_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        for seq, count in sorted_seqs[:3]:\n",
    "            if count > 1:\n",
    "                print(f\"  Count: {count}x - {seq[:30]}...\")\n",
    "    \n",
    "    return sequence_counts\n",
    "\n",
    "dup_counts = detect_duplicates(fastq_reads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 3: Read Alignment and SAM/BAM Format\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand SAM/BAM file structure\n",
    "- Parse alignment information\n",
    "- Calculate alignment statistics\n",
    "\n",
    "### üìñ Key SAM Fields\n",
    "1. **QNAME** - Read name\n",
    "2. **FLAG** - Bitwise flag (paired, mapped, etc.)\n",
    "3. **RNAME** - Reference sequence name (chromosome)\n",
    "4. **POS** - Alignment position\n",
    "5. **MAPQ** - Mapping quality score\n",
    "6. **CIGAR** - Alignment string (M=match, I=insertion, D=deletion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Simulate SAM alignment data\n",
    "def create_simulated_sam_alignments():\n",
    "    \"\"\"Generate simulated SAM alignment entries\"\"\"\n",
    "    \n",
    "    sam_data = [\n",
    "        {'QNAME': 'READ_1', 'FLAG': 99, 'RNAME': 'chr1', 'POS': 10001, 'MAPQ': 60, 'CIGAR': '76M'},\n",
    "        {'QNAME': 'READ_2', 'FLAG': 163, 'RNAME': 'chr1', 'POS': 10245, 'MAPQ': 55, 'CIGAR': '70M1I5M'},\n",
    "        {'QNAME': 'READ_3', 'FLAG': 99, 'RNAME': 'chr2', 'POS': 20500, 'MAPQ': 42, 'CIGAR': '60M2D16M'},\n",
    "        {'QNAME': 'READ_4', 'FLAG': 4, 'RNAME': '*', 'POS': 0, 'MAPQ': 0, 'CIGAR': '*'},  # Unmapped\n",
    "        {'QNAME': 'READ_5', 'FLAG': 99, 'RNAME': 'chr1', 'POS': 15780, 'MAPQ': 60, 'CIGAR': '76M'},\n",
    "    ]\n",
    "    \n",
    "    return sam_data\n",
    "\n",
    "# Create alignments\n",
    "sam_alignments = create_simulated_sam_alignments()\n",
    "\n",
    "# Display as DataFrame\n",
    "df_sam = pd.DataFrame(sam_alignments)\n",
    "print(\"SAM Alignment Data:\")\n",
    "print(\"=\" * 80)\n",
    "print(df_sam.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Parse SAM FLAGS\n",
    "def decode_sam_flag(flag):\n",
    "    \"\"\"Decode SAM bitwise flag\"\"\"\n",
    "    \n",
    "    flags = {\n",
    "        1: 'paired',\n",
    "        2: 'properly paired',\n",
    "        4: 'unmapped',\n",
    "        8: 'mate unmapped',\n",
    "        16: 'reverse strand',\n",
    "        32: 'mate reverse strand',\n",
    "        64: 'first in pair',\n",
    "        128: 'second in pair',\n",
    "        256: 'secondary alignment',\n",
    "        512: 'failed QC',\n",
    "        1024: 'duplicate',\n",
    "        2048: 'supplementary'\n",
    "    }\n",
    "    \n",
    "    active_flags = []\n",
    "    for bit, description in flags.items():\n",
    "        if flag & bit:\n",
    "            active_flags.append(description)\n",
    "    \n",
    "    return active_flags\n",
    "\n",
    "# Decode FLAGS for all alignments\n",
    "print(\"\\nFLAG Interpretation:\")\n",
    "print(\"=\" * 80)\n",
    "for aln in sam_alignments:\n",
    "    flags = decode_sam_flag(aln['FLAG'])\n",
    "    print(f\"{aln['QNAME']:10s} | FLAG={aln['FLAG']:4d} | {', '.join(flags) if flags else 'No flags'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Parse CIGAR strings\n",
    "def parse_cigar(cigar_string):\n",
    "    \"\"\"Parse CIGAR string and calculate alignment statistics\"\"\"\n",
    "    \n",
    "    if cigar_string == '*':\n",
    "        return {'matches': 0, 'insertions': 0, 'deletions': 0, 'total': 0}\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    operations = re.findall(r'(\\d+)([MIDNSHPX=])', cigar_string)\n",
    "    \n",
    "    stats = {'matches': 0, 'insertions': 0, 'deletions': 0, 'total': 0}\n",
    "    \n",
    "    for length, op in operations:\n",
    "        length = int(length)\n",
    "        if op == 'M' or op == '=':\n",
    "            stats['matches'] += length\n",
    "        elif op == 'I':\n",
    "            stats['insertions'] += length\n",
    "        elif op == 'D':\n",
    "            stats['deletions'] += length\n",
    "        \n",
    "        stats['total'] += length\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Parse CIGAR for all alignments\n",
    "print(\"\\nCIGAR String Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Read':<12} {'CIGAR':<15} {'Matches':<10} {'Insertions':<12} {'Deletions':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for aln in sam_alignments:\n",
    "    stats = parse_cigar(aln['CIGAR'])\n",
    "    print(f\"{aln['QNAME']:<12} {aln['CIGAR']:<15} {stats['matches']:<10} \"\n",
    "          f\"{stats['insertions']:<12} {stats['deletions']:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Calculate alignment statistics\n",
    "def calculate_alignment_stats(sam_data):\n",
    "    \"\"\"Calculate overall alignment statistics\"\"\"\n",
    "    \n",
    "    total_reads = len(sam_data)\n",
    "    mapped_reads = sum(1 for aln in sam_data if not (aln['FLAG'] & 4))\n",
    "    unmapped_reads = total_reads - mapped_reads\n",
    "    \n",
    "    # MAPQ distribution\n",
    "    mapq_scores = [aln['MAPQ'] for aln in sam_data if aln['MAPQ'] > 0]\n",
    "    \n",
    "    # Chromosome distribution\n",
    "    chr_counts = defaultdict(int)\n",
    "    for aln in sam_data:\n",
    "        if aln['RNAME'] != '*':\n",
    "            chr_counts[aln['RNAME']] += 1\n",
    "    \n",
    "    print(\"Alignment Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total reads: {total_reads}\")\n",
    "    print(f\"Mapped reads: {mapped_reads} ({mapped_reads/total_reads*100:.1f}%)\")\n",
    "    print(f\"Unmapped reads: {unmapped_reads} ({unmapped_reads/total_reads*100:.1f}%)\")\n",
    "    \n",
    "    if mapq_scores:\n",
    "        print(f\"\\nMapping Quality:\")\n",
    "        print(f\"  Mean MAPQ: {np.mean(mapq_scores):.2f}\")\n",
    "        print(f\"  High quality (MAPQ ‚â• 30): {sum(1 for q in mapq_scores if q >= 30)} reads\")\n",
    "    \n",
    "    print(f\"\\nChromosome Distribution:\")\n",
    "    for chr_name, count in sorted(chr_counts.items()):\n",
    "        print(f\"  {chr_name}: {count} reads\")\n",
    "    \n",
    "    return {\n",
    "        'total': total_reads,\n",
    "        'mapped': mapped_reads,\n",
    "        'unmapped': unmapped_reads,\n",
    "        'mapping_rate': mapped_reads/total_reads*100\n",
    "    }\n",
    "\n",
    "alignment_stats = calculate_alignment_stats(sam_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 4: Variant Calling and VCF Format\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand VCF file structure\n",
    "- Parse variant information\n",
    "- Calculate variant statistics\n",
    "\n",
    "### üìñ VCF Format\n",
    "```\n",
    "#CHROM  POS     ID      REF  ALT  QUAL  FILTER  INFO           FORMAT  SAMPLE\n",
    "chr1    10177   .       A    AC   50    PASS    DP=32;AF=0.5   GT:DP   0/1:32\n",
    "chr1    10352   rs123   T    A    100   PASS    DP=45;AF=1.0   GT:DP   1/1:45\n",
    "```\n",
    "\n",
    "**Genotypes:**\n",
    "- 0/0 = Homozygous reference\n",
    "- 0/1 = Heterozygous\n",
    "- 1/1 = Homozygous alternate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Create simulated VCF data\n",
    "def create_simulated_vcf():\n",
    "    \"\"\"Generate simulated VCF variant data\"\"\"\n",
    "    \n",
    "    vcf_data = [\n",
    "        {'CHROM': 'chr1', 'POS': 10177, 'ID': '.', 'REF': 'A', 'ALT': 'G', \n",
    "         'QUAL': 50, 'FILTER': 'PASS', 'DP': 32, 'AF': 0.5, 'GT': '0/1'},\n",
    "        {'CHROM': 'chr1', 'POS': 10352, 'ID': 'rs12345', 'REF': 'T', 'ALT': 'A', \n",
    "         'QUAL': 100, 'FILTER': 'PASS', 'DP': 45, 'AF': 1.0, 'GT': '1/1'},\n",
    "        {'CHROM': 'chr2', 'POS': 20543, 'ID': '.', 'REF': 'GTC', 'ALT': 'G', \n",
    "         'QUAL': 35, 'FILTER': 'LowQual', 'DP': 18, 'AF': 0.33, 'GT': '0/1'},\n",
    "        {'CHROM': 'chr2', 'POS': 20890, 'ID': 'rs67890', 'REF': 'C', 'ALT': 'T', \n",
    "         'QUAL': 85, 'FILTER': 'PASS', 'DP': 55, 'AF': 0.5, 'GT': '0/1'},\n",
    "        {'CHROM': 'chr3', 'POS': 35120, 'ID': '.', 'REF': 'A', 'ALT': 'AGTC', \n",
    "         'QUAL': 42, 'FILTER': 'PASS', 'DP': 28, 'AF': 0.5, 'GT': '0/1'},\n",
    "    ]\n",
    "    \n",
    "    return vcf_data\n",
    "\n",
    "# Create VCF data\n",
    "vcf_variants = create_simulated_vcf()\n",
    "df_vcf = pd.DataFrame(vcf_variants)\n",
    "\n",
    "print(\"VCF Variant Data:\")\n",
    "print(\"=\" * 100)\n",
    "print(df_vcf.to_string(index=False))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Classify variant types\n",
    "def classify_variant_type(ref, alt):\n",
    "    \"\"\"Classify variant as SNV, insertion, or deletion\"\"\"\n",
    "    \n",
    "    ref_len = len(ref)\n",
    "    alt_len = len(alt)\n",
    "    \n",
    "    if ref_len == alt_len == 1:\n",
    "        return 'SNV'\n",
    "    elif ref_len < alt_len:\n",
    "        return 'Insertion'\n",
    "    elif ref_len > alt_len:\n",
    "        return 'Deletion'\n",
    "    else:\n",
    "        return 'Complex'\n",
    "\n",
    "def classify_genotype(gt):\n",
    "    \"\"\"Classify genotype as homozygous ref, het, or homozygous alt\"\"\"\n",
    "    \n",
    "    if gt == '0/0':\n",
    "        return 'Homozygous Reference'\n",
    "    elif gt == '0/1' or gt == '1/0':\n",
    "        return 'Heterozygous'\n",
    "    elif gt == '1/1':\n",
    "        return 'Homozygous Alternate'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Add classifications\n",
    "df_vcf['Variant_Type'] = df_vcf.apply(lambda row: classify_variant_type(row['REF'], row['ALT']), axis=1)\n",
    "df_vcf['Genotype_Class'] = df_vcf['GT'].apply(classify_genotype)\n",
    "\n",
    "print(\"\\nVariant Classification:\")\n",
    "print(\"=\" * 100)\n",
    "print(df_vcf[['CHROM', 'POS', 'REF', 'ALT', 'Variant_Type', 'GT', 'Genotype_Class']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Analyze variant statistics\n",
    "def analyze_variant_stats(vcf_data):\n",
    "    \"\"\"Calculate comprehensive variant statistics\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(vcf_data)\n",
    "    \n",
    "    # Add variant types\n",
    "    df['Variant_Type'] = df.apply(lambda row: classify_variant_type(row['REF'], row['ALT']), axis=1)\n",
    "    \n",
    "    total_variants = len(df)\n",
    "    passed_variants = len(df[df['FILTER'] == 'PASS'])\n",
    "    \n",
    "    print(\"Variant Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total variants: {total_variants}\")\n",
    "    print(f\"Passed filters: {passed_variants} ({passed_variants/total_variants*100:.1f}%)\")\n",
    "    print(f\"Failed filters: {total_variants - passed_variants}\")\n",
    "    \n",
    "    # Variant type distribution\n",
    "    print(\"\\nVariant Type Distribution:\")\n",
    "    type_counts = df['Variant_Type'].value_counts()\n",
    "    for vtype, count in type_counts.items():\n",
    "        print(f\"  {vtype}: {count} ({count/total_variants*100:.1f}%)\")\n",
    "    \n",
    "    # Chromosome distribution\n",
    "    print(\"\\nChromosome Distribution:\")\n",
    "    chr_counts = df['CHROM'].value_counts().sort_index()\n",
    "    for chrom, count in chr_counts.items():\n",
    "        print(f\"  {chrom}: {count} variants\")\n",
    "    \n",
    "    # Quality statistics\n",
    "    print(\"\\nQuality Statistics:\")\n",
    "    print(f\"  Mean QUAL: {df['QUAL'].mean():.2f}\")\n",
    "    print(f\"  Mean Depth: {df['DP'].mean():.2f}\")\n",
    "    print(f\"  Mean Allele Frequency: {df['AF'].mean():.2f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_variants = analyze_variant_stats(vcf_variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Visualize variant distributions\n",
    "def visualize_variants(df):\n",
    "    \"\"\"Create visualizations for variant data\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Variant type distribution\n",
    "    type_counts = df['Variant_Type'].value_counts()\n",
    "    axes[0, 0].bar(type_counts.index, type_counts.values, color=['steelblue', 'coral', 'lightgreen'])\n",
    "    axes[0, 0].set_xlabel('Variant Type')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].set_title('Variant Type Distribution')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Quality score distribution\n",
    "    axes[0, 1].hist(df['QUAL'], bins=10, color='purple', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].axvline(x=30, color='red', linestyle='--', linewidth=2, label='Q30 threshold')\n",
    "    axes[0, 1].set_xlabel('Quality Score')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Variant Quality Distribution')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Depth vs Allele Frequency\n",
    "    passed = df[df['FILTER'] == 'PASS']\n",
    "    failed = df[df['FILTER'] != 'PASS']\n",
    "    axes[1, 0].scatter(passed['DP'], passed['AF'], color='green', s=100, alpha=0.6, label='PASS')\n",
    "    axes[1, 0].scatter(failed['DP'], failed['AF'], color='red', s=100, alpha=0.6, label='Failed')\n",
    "    axes[1, 0].set_xlabel('Depth (DP)')\n",
    "    axes[1, 0].set_ylabel('Allele Frequency (AF)')\n",
    "    axes[1, 0].set_title('Depth vs Allele Frequency')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Chromosome distribution\n",
    "    chr_counts = df['CHROM'].value_counts().sort_index()\n",
    "    axes[1, 1].bar(chr_counts.index, chr_counts.values, color='teal', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Chromosome')\n",
    "    axes[1, 1].set_ylabel('Number of Variants')\n",
    "    axes[1, 1].set_title('Variants per Chromosome')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Variant visualization complete!\")\n",
    "\n",
    "visualize_variants(df_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 5: Variant Annotation and Interpretation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand variant annotation databases\n",
    "- Predict functional effects\n",
    "- Filter and prioritize variants\n",
    "\n",
    "### üìñ Key Annotation Sources\n",
    "- **gnomAD**: Population frequencies\n",
    "- **ClinVar**: Clinical significance\n",
    "- **dbSNP**: Known variant database\n",
    "- **SIFT/PolyPhen**: Deleteriousness prediction\n",
    "- **CADD**: Combined prediction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Add simulated annotations\n",
    "def add_variant_annotations(vcf_data):\n",
    "    \"\"\"Add simulated functional annotations to variants\"\"\"\n",
    "    \n",
    "    annotations = [\n",
    "        {'Gene': 'BRCA1', 'Effect': 'missense_variant', 'Impact': 'MODERATE', \n",
    "         'gnomAD_AF': 0.0001, 'SIFT': 'deleterious', 'ClinVar': 'Pathogenic'},\n",
    "        {'Gene': 'TP53', 'Effect': 'synonymous_variant', 'Impact': 'LOW', \n",
    "         'gnomAD_AF': 0.15, 'SIFT': 'tolerated', 'ClinVar': 'Benign'},\n",
    "        {'Gene': 'KRAS', 'Effect': 'frameshift_variant', 'Impact': 'HIGH', \n",
    "         'gnomAD_AF': None, 'SIFT': 'deleterious', 'ClinVar': 'Likely_pathogenic'},\n",
    "        {'Gene': 'EGFR', 'Effect': 'missense_variant', 'Impact': 'MODERATE', \n",
    "         'gnomAD_AF': 0.002, 'SIFT': 'deleterious', 'ClinVar': 'VUS'},\n",
    "        {'Gene': 'PIK3CA', 'Effect': 'inframe_insertion', 'Impact': 'MODERATE', \n",
    "         'gnomAD_AF': 0.0005, 'SIFT': 'deleterious', 'ClinVar': 'Pathogenic'},\n",
    "    ]\n",
    "    \n",
    "    # Merge with VCF data\n",
    "    df_vcf = pd.DataFrame(vcf_data)\n",
    "    df_ann = pd.DataFrame(annotations)\n",
    "    df_merged = pd.concat([df_vcf, df_ann], axis=1)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "# Add annotations\n",
    "df_annotated = add_variant_annotations(vcf_variants)\n",
    "\n",
    "print(\"Annotated Variants:\")\n",
    "print(\"=\" * 120)\n",
    "display_cols = ['CHROM', 'POS', 'REF', 'ALT', 'Gene', 'Effect', 'Impact', 'gnomAD_AF', 'SIFT', 'ClinVar']\n",
    "print(df_annotated[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Filter and prioritize variants\n",
    "def prioritize_variants(df):\n",
    "    \"\"\"Filter variants based on clinical relevance criteria\"\"\"\n",
    "    \n",
    "    print(\"Variant Prioritization Pipeline:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total = len(df)\n",
    "    print(f\"Starting variants: {total}\")\n",
    "    \n",
    "    # Filter 1: Quality\n",
    "    df_filtered = df[df['FILTER'] == 'PASS'].copy()\n",
    "    print(f\"After quality filter (PASS): {len(df_filtered)} ({len(df_filtered)/total*100:.1f}%)\")\n",
    "    \n",
    "    # Filter 2: Impact\n",
    "    df_filtered = df_filtered[df_filtered['Impact'].isin(['HIGH', 'MODERATE'])]\n",
    "    print(f\"After impact filter (HIGH/MODERATE): {len(df_filtered)} ({len(df_filtered)/total*100:.1f}%)\")\n",
    "    \n",
    "    # Filter 3: Population frequency (rare variants)\n",
    "    df_filtered = df_filtered[\n",
    "        (df_filtered['gnomAD_AF'].isna()) | (df_filtered['gnomAD_AF'] < 0.01)\n",
    "    ]\n",
    "    print(f\"After frequency filter (AF < 1%): {len(df_filtered)} ({len(df_filtered)/total*100:.1f}%)\")\n",
    "    \n",
    "    # Filter 4: Deleteriousness prediction\n",
    "    df_filtered = df_filtered[df_filtered['SIFT'] == 'deleterious']\n",
    "    print(f\"After SIFT filter (deleterious): {len(df_filtered)} ({len(df_filtered)/total*100:.1f}%)\")\n",
    "    \n",
    "    # Prioritize by ClinVar significance\n",
    "    priority_order = ['Pathogenic', 'Likely_pathogenic', 'VUS']\n",
    "    df_filtered['Priority'] = df_filtered['ClinVar'].map(\n",
    "        {val: idx for idx, val in enumerate(priority_order)}\n",
    "    )\n",
    "    df_filtered = df_filtered.sort_values('Priority')\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Prioritized Variants (Highest to Lowest):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    display_cols = ['Gene', 'Effect', 'Impact', 'gnomAD_AF', 'SIFT', 'ClinVar']\n",
    "    print(df_filtered[display_cols].to_string(index=False))\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "df_prioritized = prioritize_variants(df_annotated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 6: RNA-seq Data Analysis\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand RNA-seq count data\n",
    "- Perform differential expression analysis\n",
    "- Visualize gene expression patterns\n",
    "\n",
    "### üìñ Key Concepts\n",
    "- **TPM/FPKM**: Normalized expression values\n",
    "- **Fold Change**: Expression ratio between conditions\n",
    "- **P-value**: Statistical significance\n",
    "- **FDR**: Multiple testing correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Create simulated RNA-seq count data\n",
    "def create_simulated_rnaseq_data():\n",
    "    \"\"\"Generate simulated RNA-seq expression data\"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    genes = ['BRCA1', 'TP53', 'EGFR', 'MYC', 'KRAS', 'PIK3CA', 'PTEN', 'AKT1', 'BRAF', 'NRAS']\n",
    "    \n",
    "    # Simulate counts for control vs treatment\n",
    "    data = []\n",
    "    for gene in genes:\n",
    "        # Base expression level\n",
    "        base_count = np.random.randint(100, 5000)\n",
    "        \n",
    "        # Control samples (n=3)\n",
    "        control_counts = np.random.poisson(base_count, 3)\n",
    "        \n",
    "        # Treatment samples (n=3) - some genes up/downregulated\n",
    "        fold_change = np.random.choice([0.5, 0.8, 1.0, 1.5, 2.0, 3.0])\n",
    "        treatment_counts = np.random.poisson(base_count * fold_change, 3)\n",
    "        \n",
    "        data.append({\n",
    "            'Gene': gene,\n",
    "            'Control_1': control_counts[0],\n",
    "            'Control_2': control_counts[1],\n",
    "            'Control_3': control_counts[2],\n",
    "            'Treatment_1': treatment_counts[0],\n",
    "            'Treatment_2': treatment_counts[1],\n",
    "            'Treatment_3': treatment_counts[2],\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Calculate mean expression\n",
    "    df['Control_Mean'] = df[['Control_1', 'Control_2', 'Control_3']].mean(axis=1)\n",
    "    df['Treatment_Mean'] = df[['Treatment_1', 'Treatment_2', 'Treatment_3']].mean(axis=1)\n",
    "    \n",
    "    # Calculate fold change\n",
    "    df['Fold_Change'] = df['Treatment_Mean'] / df['Control_Mean']\n",
    "    df['Log2_FC'] = np.log2(df['Fold_Change'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "df_rnaseq = create_simulated_rnaseq_data()\n",
    "\n",
    "print(\"RNA-seq Expression Data:\")\n",
    "print(\"=\" * 100)\n",
    "display_cols = ['Gene', 'Control_Mean', 'Treatment_Mean', 'Fold_Change', 'Log2_FC']\n",
    "print(df_rnaseq[display_cols].round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Perform differential expression analysis\n",
    "def differential_expression_analysis(df):\n",
    "    \"\"\"Simple differential expression analysis using t-test\"\"\"\n",
    "    \n",
    "    from scipy import stats\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        control_samples = [row['Control_1'], row['Control_2'], row['Control_3']]\n",
    "        treatment_samples = [row['Treatment_1'], row['Treatment_2'], row['Treatment_3']]\n",
    "        \n",
    "        # Perform t-test\n",
    "        t_stat, p_value = stats.ttest_ind(control_samples, treatment_samples)\n",
    "        \n",
    "        results.append({\n",
    "            'Gene': row['Gene'],\n",
    "            'Log2_FC': row['Log2_FC'],\n",
    "            'P_value': p_value,\n",
    "            'Significant': 'Yes' if p_value < 0.05 else 'No',\n",
    "            'Direction': 'Up' if row['Log2_FC'] > 0 else 'Down'\n",
    "        })\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"Differential Expression Results:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df_results.round(4).to_string(index=False))\n",
    "    \n",
    "    # Summary\n",
    "    sig_count = (df_results['P_value'] < 0.05).sum()\n",
    "    up_count = ((df_results['P_value'] < 0.05) & (df_results['Log2_FC'] > 0)).sum()\n",
    "    down_count = ((df_results['P_value'] < 0.05) & (df_results['Log2_FC'] < 0)).sum()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Significant genes (p < 0.05): {sig_count}/{len(df_results)}\")\n",
    "    print(f\"  Upregulated: {up_count}\")\n",
    "    print(f\"  Downregulated: {down_count}\")\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "df_de = differential_expression_analysis(df_rnaseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Visualize differential expression\n",
    "def visualize_differential_expression(df_de):\n",
    "    \"\"\"Create volcano plot and expression heatmap\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Volcano plot\n",
    "    df_de['neg_log10_p'] = -np.log10(df_de['P_value'])\n",
    "    \n",
    "    # Color coding\n",
    "    colors = []\n",
    "    for _, row in df_de.iterrows():\n",
    "        if row['P_value'] < 0.05 and abs(row['Log2_FC']) > 1:\n",
    "            colors.append('red' if row['Log2_FC'] > 0 else 'blue')\n",
    "        else:\n",
    "            colors.append('gray')\n",
    "    \n",
    "    ax1.scatter(df_de['Log2_FC'], df_de['neg_log10_p'], c=colors, s=100, alpha=0.6)\n",
    "    ax1.axhline(y=-np.log10(0.05), color='green', linestyle='--', linewidth=2, label='p = 0.05')\n",
    "    ax1.axvline(x=1, color='orange', linestyle='--', linewidth=2, label='FC = 2')\n",
    "    ax1.axvline(x=-1, color='orange', linestyle='--', linewidth=2)\n",
    "    \n",
    "    # Annotate significant genes\n",
    "    for _, row in df_de.iterrows():\n",
    "        if row['P_value'] < 0.05 and abs(row['Log2_FC']) > 1:\n",
    "            ax1.text(row['Log2_FC'], row['neg_log10_p'], row['Gene'], \n",
    "                    fontsize=9, ha='right')\n",
    "    \n",
    "    ax1.set_xlabel('Log2 Fold Change', fontsize=12)\n",
    "    ax1.set_ylabel('-Log10 P-value', fontsize=12)\n",
    "    ax1.set_title('Volcano Plot', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Expression bar plot\n",
    "    df_sig = df_de[df_de['P_value'] < 0.05].sort_values('Log2_FC')\n",
    "    if len(df_sig) > 0:\n",
    "        colors_bar = ['red' if x > 0 else 'blue' for x in df_sig['Log2_FC']]\n",
    "        ax2.barh(df_sig['Gene'], df_sig['Log2_FC'], color=colors_bar, alpha=0.7)\n",
    "        ax2.axvline(x=0, color='black', linewidth=1)\n",
    "        ax2.set_xlabel('Log2 Fold Change', fontsize=12)\n",
    "        ax2.set_ylabel('Gene', fontsize=12)\n",
    "        ax2.set_title('Significant Differentially Expressed Genes', fontsize=14, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Differential expression visualization complete!\")\n",
    "\n",
    "visualize_differential_expression(df_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Practice Complete!\n",
    "\n",
    "### Summary of What We Learned:\n",
    "\n",
    "1. **FASTQ Format**: Understanding quality scores and Phred encoding\n",
    "2. **Quality Control**: Analyzing read quality, GC content, and duplicates\n",
    "3. **SAM/BAM Format**: Parsing alignment data and calculating statistics\n",
    "4. **Variant Calling**: Understanding VCF format and variant classification\n",
    "5. **Variant Annotation**: Functional prediction and clinical interpretation\n",
    "6. **RNA-seq Analysis**: Gene expression quantification and differential expression\n",
    "\n",
    "### Key Insights:\n",
    "- NGS data analysis follows a structured pipeline from raw reads to biological insights\n",
    "- Quality control is critical at every step of the analysis\n",
    "- Proper annotation and interpretation are essential for clinical applications\n",
    "- Understanding file formats (FASTQ, SAM/BAM, VCF) is fundamental\n",
    "\n",
    "### Next Steps:\n",
    "- Practice with real NGS datasets from public databases (NCBI SRA, ENA)\n",
    "- Learn command-line tools: FastQC, BWA, GATK, SAMtools\n",
    "- Explore Galaxy platform for web-based analysis\n",
    "- Try advanced analyses: ChIP-seq, ATAC-seq, single-cell RNA-seq\n",
    "\n",
    "### Resources:\n",
    "- **Galaxy**: https://usegalaxy.org\n",
    "- **GATK Best Practices**: https://gatk.broadinstitute.org\n",
    "- **Bioconductor**: https://bioconductor.org\n",
    "- **Training Materials**: https://training.galaxyproject.org\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for completing this hands-on practice!** üß¨üî¨\n",
    "\n",
    "For questions or feedback, contact:  \n",
    "**Ho-min Park**  \n",
    "üìß homin.park@ghent.ac.kr | powersimmani@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
