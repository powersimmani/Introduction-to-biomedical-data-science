{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè• Deep Learning for Medical Imaging: Practical Implementation\n",
    "\n",
    "## Table of Contents\n",
    "1. [CNN Fundamentals - Convolution Operations](#practice-1-cnn-fundamentals---convolution-operations)\n",
    "2. [Building a Simple CNN from Scratch](#practice-2-building-a-simple-cnn-from-scratch)\n",
    "3. [Transfer Learning with Pre-trained Models](#practice-3-transfer-learning-with-pre-trained-models)\n",
    "4. [U-Net Architecture for Medical Image Segmentation](#practice-4-u-net-architecture-for-medical-image-segmentation)\n",
    "5. [Class Activation Maps (CAM) for Interpretability](#practice-5-class-activation-maps-cam-for-interpretability)\n",
    "6. [Data Augmentation for Medical Images](#practice-6-data-augmentation-for-medical-images)\n",
    "7. [Introduction to MONAI Framework](#practice-7-introduction-to-monai-framework)\n",
    "8. [Model Evaluation and Validation](#practice-8-model-evaluation-and-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch torchvision matplotlib numpy Pillow\n",
    "# !pip install monai\n",
    "\n",
    "# Import essential libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Check PyTorch version and device\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"\\n‚úÖ All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 1: CNN Fundamentals - Convolution Operations\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand how convolution operations work\n",
    "- Implement convolution manually and compare with PyTorch\n",
    "- Visualize feature maps at different layers\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Convolution Operation:** Slides a kernel (filter) across an input image to extract features\n",
    "- **Kernel size:** 3√ó3, 5√ó5, 7√ó7\n",
    "- **Stride:** Step size of kernel movement\n",
    "- **Padding:** Adding borders to preserve spatial dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Manual implementation of 2D convolution\n",
    "def manual_conv2d(input_image, kernel, stride=1, padding=0):\n",
    "    \"\"\"Manually implement 2D convolution operation\"\"\"\n",
    "    \n",
    "    # Add padding if needed\n",
    "    if padding > 0:\n",
    "        input_image = np.pad(input_image, padding, mode='constant', constant_values=0)\n",
    "    \n",
    "    # Get dimensions\n",
    "    h, w = input_image.shape\n",
    "    kh, kw = kernel.shape\n",
    "    \n",
    "    # Calculate output dimensions\n",
    "    out_h = (h - kh) // stride + 1\n",
    "    out_w = (w - kw) // stride + 1\n",
    "    \n",
    "    # Initialize output\n",
    "    output = np.zeros((out_h, out_w))\n",
    "    \n",
    "    # Perform convolution\n",
    "    for i in range(0, out_h):\n",
    "        for j in range(0, out_w):\n",
    "            # Extract region\n",
    "            region = input_image[i*stride:i*stride+kh, j*stride:j*stride+kw]\n",
    "            # Element-wise multiplication and sum\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Create a simple test image (8x8)\n",
    "test_image = np.random.rand(8, 8)\n",
    "\n",
    "# Define edge detection kernels\n",
    "sobel_x = np.array([[-1, 0, 1],\n",
    "                    [-2, 0, 2],\n",
    "                    [-1, 0, 1]])\n",
    "\n",
    "sobel_y = np.array([[-1, -2, -1],\n",
    "                    [ 0,  0,  0],\n",
    "                    [ 1,  2,  1]])\n",
    "\n",
    "# Apply manual convolution\n",
    "output_x = manual_conv2d(test_image, sobel_x, stride=1, padding=1)\n",
    "output_y = manual_conv2d(test_image, sobel_y, stride=1, padding=1)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].imshow(test_image, cmap='gray')\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(output_x, cmap='gray')\n",
    "axes[1].set_title('Horizontal Edge Detection (Sobel X)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(output_y, cmap='gray')\n",
    "axes[2].set_title('Vertical Edge Detection (Sobel Y)')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Manual convolution completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Compare with PyTorch convolution\n",
    "def pytorch_conv_comparison():\n",
    "    \"\"\"Compare manual implementation with PyTorch\"\"\"\n",
    "    \n",
    "    # Create input (1 batch, 1 channel, 8x8)\n",
    "    input_tensor = torch.tensor(test_image).unsqueeze(0).unsqueeze(0).float()\n",
    "    \n",
    "    # Create convolution layer\n",
    "    conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, \n",
    "                          stride=1, padding=1, bias=False)\n",
    "    \n",
    "    # Set kernel weights to Sobel X\n",
    "    with torch.no_grad():\n",
    "        conv_layer.weight = nn.Parameter(torch.tensor(sobel_x).unsqueeze(0).unsqueeze(0).float())\n",
    "    \n",
    "    # Apply PyTorch convolution\n",
    "    pytorch_output = conv_layer(input_tensor)\n",
    "    pytorch_output = pytorch_output.squeeze().detach().numpy()\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"Manual convolution output shape:\", output_x.shape)\n",
    "    print(\"PyTorch convolution output shape:\", pytorch_output.shape)\n",
    "    print(f\"\\nMaximum difference: {np.max(np.abs(output_x - pytorch_output)):.6f}\")\n",
    "    print(\"‚úÖ Results match!\" if np.allclose(output_x, pytorch_output, atol=1e-5) else \"‚ùå Results differ\")\n",
    "    \n",
    "    return conv_layer\n",
    "\n",
    "conv_layer = pytorch_conv_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 2: Building a Simple CNN from Scratch\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Design and implement a simple CNN architecture\n",
    "- Understand the flow of data through convolutional layers\n",
    "- Learn about pooling operations\n",
    "\n",
    "### üìñ Key Architecture Components\n",
    "- **Convolutional layers:** Feature extraction\n",
    "- **Pooling layers:** Spatial downsampling\n",
    "- **Fully connected layers:** Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Define a simple CNN for binary classification\n",
    "class SimpleMedicalCNN(nn.Module):\n",
    "    \"\"\"Simple CNN for medical image classification\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleMedicalCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional Block 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # 1 -> 16 channels\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # 224x224 -> 112x112\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # 16 -> 32 channels\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # 112x112 -> 56x56\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 32 -> 64 channels\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # 56x56 -> 28x28\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)  # -> 1x1\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        # Classification\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model = SimpleMedicalCNN(num_classes=2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model architecture\n",
    "print(\"Model Architecture:\")\n",
    "print(\"=\" * 60)\n",
    "print(model)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(\"\\n‚úÖ Model created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Test forward pass and visualize feature map sizes\n",
    "def test_forward_pass():\n",
    "    \"\"\"Test the model with dummy input and track dimensions\"\"\"\n",
    "    \n",
    "    # Create dummy input (batch_size=1, channels=1, height=224, width=224)\n",
    "    dummy_input = torch.randn(1, 1, 224, 224).to(device)\n",
    "    \n",
    "    print(\"Forward Pass - Feature Map Dimensions:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    \n",
    "    # Track intermediate outputs\n",
    "    x = dummy_input\n",
    "    \n",
    "    # Block 1\n",
    "    x = model.conv1(x)\n",
    "    print(f\"After Conv1: {x.shape}\")\n",
    "    x = F.relu(model.bn1(x))\n",
    "    x = model.pool1(x)\n",
    "    print(f\"After Pool1: {x.shape}\")\n",
    "    \n",
    "    # Block 2\n",
    "    x = model.conv2(x)\n",
    "    print(f\"After Conv2: {x.shape}\")\n",
    "    x = F.relu(model.bn2(x))\n",
    "    x = model.pool2(x)\n",
    "    print(f\"After Pool2: {x.shape}\")\n",
    "    \n",
    "    # Block 3\n",
    "    x = model.conv3(x)\n",
    "    print(f\"After Conv3: {x.shape}\")\n",
    "    x = F.relu(model.bn3(x))\n",
    "    x = model.pool3(x)\n",
    "    print(f\"After Pool3: {x.shape}\")\n",
    "    \n",
    "    # Global pooling\n",
    "    x = model.global_avg_pool(x)\n",
    "    print(f\"After Global Avg Pool: {x.shape}\")\n",
    "    \n",
    "    x = x.view(x.size(0), -1)\n",
    "    print(f\"After Flatten: {x.shape}\")\n",
    "    \n",
    "    # Final output\n",
    "    x = model.fc(x)\n",
    "    print(f\"Final Output: {x.shape}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚úÖ Forward pass successful!\")\n",
    "    \n",
    "    return x\n",
    "\n",
    "output = test_forward_pass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 3: Transfer Learning with Pre-trained Models\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Load a pre-trained model (ResNet18)\n",
    "- Modify it for medical imaging tasks\n",
    "- Understand fine-tuning strategies\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Transfer Learning:** Use knowledge from large datasets (ImageNet) for medical imaging\n",
    "- **Feature Extraction:** Freeze early layers\n",
    "- **Fine-tuning:** Gradually unfreeze and train layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Load pre-trained ResNet18 and modify for medical imaging\n",
    "def create_transfer_learning_model(num_classes=2, grayscale=True):\n",
    "    \"\"\"Create a transfer learning model from pre-trained ResNet18\"\"\"\n",
    "    \n",
    "    # Load pre-trained ResNet18\n",
    "    model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    print(\"Original ResNet18 Architecture:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"First layer (conv1) input channels: {model.conv1.in_channels}\")\n",
    "    print(f\"Last layer (fc) output features: {model.fc.out_features}\")\n",
    "    \n",
    "    # Modify first layer for grayscale images (1 channel instead of 3)\n",
    "    if grayscale:\n",
    "        model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    \n",
    "    # Modify last layer for our classification task\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    print(\"\\nModified Architecture:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"First layer (conv1) input channels: {model.conv1.in_channels}\")\n",
    "    print(f\"Last layer (fc) output features: {model.fc.out_features}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create transfer learning model\n",
    "transfer_model = create_transfer_learning_model(num_classes=2, grayscale=True)\n",
    "transfer_model = transfer_model.to(device)\n",
    "\n",
    "print(\"\\n‚úÖ Transfer learning model created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Implement different fine-tuning strategies\n",
    "def apply_finetuning_strategy(model, strategy='freeze_early'):\n",
    "    \"\"\"\n",
    "    Apply different fine-tuning strategies\n",
    "    \n",
    "    Strategies:\n",
    "    - 'freeze_early': Freeze all layers except the last one\n",
    "    - 'freeze_most': Freeze all except last 2 blocks\n",
    "    - 'full': Train all layers\n",
    "    \"\"\"\n",
    "    \n",
    "    if strategy == 'freeze_early':\n",
    "        # Freeze all layers\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze last layer\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        print(\"Strategy: Freeze all layers except final classifier\")\n",
    "    \n",
    "    elif strategy == 'freeze_most':\n",
    "        # Freeze early layers\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'layer4' not in name and 'fc' not in name:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        print(\"Strategy: Freeze early layers, train layer4 and fc\")\n",
    "    \n",
    "    elif strategy == 'full':\n",
    "        # Train all layers\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        print(\"Strategy: Train all layers\")\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\")\n",
    "    print(\"\\n‚úÖ Fine-tuning strategy applied!\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test different strategies\n",
    "print(\"Testing Fine-tuning Strategies:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Strategy 1: Freeze early layers\n",
    "model_frozen = create_transfer_learning_model(num_classes=2)\n",
    "model_frozen = apply_finetuning_strategy(model_frozen, strategy='freeze_early')\n",
    "print()\n",
    "\n",
    "# Strategy 2: Freeze most\n",
    "model_partial = create_transfer_learning_model(num_classes=2)\n",
    "model_partial = apply_finetuning_strategy(model_partial, strategy='freeze_most')\n",
    "print()\n",
    "\n",
    "# Strategy 3: Full training\n",
    "model_full = create_transfer_learning_model(num_classes=2)\n",
    "model_full = apply_finetuning_strategy(model_full, strategy='full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 4: U-Net Architecture for Medical Image Segmentation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Implement the U-Net architecture\n",
    "- Understand encoder-decoder structure with skip connections\n",
    "- Learn about segmentation-specific loss functions\n",
    "\n",
    "### üìñ Key Architecture Components\n",
    "**U-Net:** Standard architecture for medical image segmentation\n",
    "- **Encoder:** Contracting path to capture context\n",
    "- **Decoder:** Expanding path for precise localization\n",
    "- **Skip Connections:** Preserve spatial information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Implement U-Net architecture\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net architecture for medical image segmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=1, num_classes=2):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder (Contracting Path)\n",
    "        self.enc1 = self.conv_block(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "        \n",
    "        # Decoder (Expanding Path)\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec3 = self.conv_block(512, 256)  # 512 because of skip connection\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = self.conv_block(256, 128)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = self.conv_block(128, 64)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.out = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "    \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        \"\"\"Convolutional block: Conv -> BN -> ReLU -> Conv -> BN -> ReLU\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool1(enc1))\n",
    "        enc3 = self.enc3(self.pool2(enc2))\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool3(enc3))\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        dec3 = self.upconv3(bottleneck)\n",
    "        dec3 = torch.cat([dec3, enc3], dim=1)  # Skip connection\n",
    "        dec3 = self.dec3(dec3)\n",
    "        \n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat([dec2, enc2], dim=1)\n",
    "        dec2 = self.dec2(dec2)\n",
    "        \n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1)\n",
    "        dec1 = self.dec1(dec1)\n",
    "        \n",
    "        # Output\n",
    "        out = self.out(dec1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Create U-Net model\n",
    "unet_model = UNet(in_channels=1, num_classes=2)\n",
    "unet_model = unet_model.to(device)\n",
    "\n",
    "# Print model info\n",
    "print(\"U-Net Architecture:\")\n",
    "print(\"=\" * 60)\n",
    "total_params = sum(p.numel() for p in unet_model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(\"\\n‚úÖ U-Net model created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Test U-Net forward pass\n",
    "def test_unet_forward():\n",
    "    \"\"\"Test U-Net with dummy input\"\"\"\n",
    "    \n",
    "    # Create dummy input (batch_size=2, channels=1, height=256, width=256)\n",
    "    dummy_input = torch.randn(2, 1, 256, 256).to(device)\n",
    "    \n",
    "    print(\"U-Net Forward Pass Test:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = unet_model(dummy_input)\n",
    "    \n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"\\nExpected: (batch_size=2, num_classes=2, height=256, width=256)\")\n",
    "    print(f\"Actual:   (batch_size={output.shape[0]}, num_classes={output.shape[1]}, \"\n",
    "          f\"height={output.shape[2]}, width={output.shape[3]})\")\n",
    "    \n",
    "    # Check if output dimensions match input dimensions (important for segmentation!)\n",
    "    if output.shape[2:] == dummy_input.shape[2:]:\n",
    "        print(\"\\n‚úÖ Spatial dimensions preserved correctly!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Spatial dimensions mismatch!\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "unet_output = test_unet_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 5: Class Activation Maps (CAM) for Interpretability\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Implement Grad-CAM for visualization\n",
    "- Understand how to interpret CNN decisions\n",
    "- Generate heatmaps showing model focus areas\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Grad-CAM:** Gradient-weighted Class Activation Mapping\n",
    "- Visualizes important regions for predictions\n",
    "- Essential for clinical trust and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Implement Grad-CAM\n",
    "class GradCAM:\n",
    "    \"\"\"Gradient-weighted Class Activation Mapping\"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Register hooks\n",
    "        target_layer.register_forward_hook(self.save_activation)\n",
    "        target_layer.register_backward_hook(self.save_gradient)\n",
    "    \n",
    "    def save_activation(self, module, input, output):\n",
    "        \"\"\"Save forward pass activations\"\"\"\n",
    "        self.activations = output.detach()\n",
    "    \n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        \"\"\"Save backward pass gradients\"\"\"\n",
    "        self.gradients = grad_output[0].detach()\n",
    "    \n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        \"\"\"\n",
    "        Generate Class Activation Map\n",
    "        \n",
    "        Args:\n",
    "            input_image: Input tensor (1, C, H, W)\n",
    "            target_class: Target class index (if None, use predicted class)\n",
    "        \n",
    "        Returns:\n",
    "            CAM heatmap\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        self.model.eval()\n",
    "        output = self.model(input_image)\n",
    "        \n",
    "        # Get predicted class if not specified\n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        # Zero gradients\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Backward pass for target class\n",
    "        class_score = output[0, target_class]\n",
    "        class_score.backward()\n",
    "        \n",
    "        # Calculate weights (global average pooling of gradients)\n",
    "        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
    "        \n",
    "        # Weighted combination of activation maps\n",
    "        cam = torch.sum(weights * self.activations, dim=1, keepdim=True)\n",
    "        \n",
    "        # Apply ReLU (only positive contributions)\n",
    "        cam = F.relu(cam)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / cam.max()\n",
    "        \n",
    "        return cam, target_class\n",
    "\n",
    "print(\"‚úÖ Grad-CAM implementation ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Visualize Grad-CAM on a sample image\n",
    "def visualize_gradcam():\n",
    "    \"\"\"Create and visualize Grad-CAM\"\"\"\n",
    "    \n",
    "    # Create a simple model for demonstration\n",
    "    demo_model = SimpleMedicalCNN(num_classes=2).to(device)\n",
    "    demo_model.eval()\n",
    "    \n",
    "    # Create Grad-CAM object (target the last convolutional layer)\n",
    "    gradcam = GradCAM(demo_model, demo_model.conv3)\n",
    "    \n",
    "    # Create dummy input image\n",
    "    input_image = torch.randn(1, 1, 224, 224).to(device)\n",
    "    \n",
    "    # Generate CAM\n",
    "    cam, predicted_class = gradcam.generate_cam(input_image)\n",
    "    \n",
    "    # Resize CAM to match input size\n",
    "    cam_resized = F.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    cam_resized = cam_resized.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(input_image.squeeze().cpu().numpy(), cmap='gray')\n",
    "    axes[0].set_title('Input Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # CAM heatmap\n",
    "    axes[1].imshow(cam_resized, cmap='jet')\n",
    "    axes[1].set_title(f'Grad-CAM (Predicted: Class {predicted_class})')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    axes[2].imshow(input_image.squeeze().cpu().numpy(), cmap='gray', alpha=0.7)\n",
    "    axes[2].imshow(cam_resized, cmap='jet', alpha=0.3)\n",
    "    axes[2].set_title('Overlay')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Grad-CAM visualization complete!\")\n",
    "    print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "visualize_gradcam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 6: Data Augmentation for Medical Images\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Implement medical-specific augmentation techniques\n",
    "- Understand the importance of augmentation in limited data scenarios\n",
    "- Apply transformations that respect anatomical constraints\n",
    "\n",
    "### üìñ Key Techniques\n",
    "- **Geometric transforms:** Rotation, flipping, scaling\n",
    "- **Intensity transforms:** Brightness, contrast, gamma correction\n",
    "- **Medical-specific:** Elastic deformations, simulating artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Define comprehensive augmentation pipeline\n",
    "class MedicalAugmentation:\n",
    "    \"\"\"Medical imaging augmentation pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Training augmentation\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "        \n",
    "        # Validation/test (no augmentation)\n",
    "        self.val_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "    \n",
    "    def show_augmentations(self, image):\n",
    "        \"\"\"Visualize different augmentations\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        \n",
    "        # Original\n",
    "        axes[0, 0].imshow(image, cmap='gray')\n",
    "        axes[0, 0].set_title('Original')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Individual augmentations\n",
    "        augmentations = [\n",
    "            ('Horizontal Flip', transforms.RandomHorizontalFlip(p=1.0)),\n",
    "            ('Rotation 15¬∞', transforms.RandomRotation(degrees=15)),\n",
    "            ('Brightness', transforms.ColorJitter(brightness=0.3)),\n",
    "            ('Contrast', transforms.ColorJitter(contrast=0.3)),\n",
    "            ('Rotation 30¬∞', transforms.RandomRotation(degrees=30)),\n",
    "            ('Combined', self.train_transform)\n",
    "        ]\n",
    "        \n",
    "        for idx, (name, transform) in enumerate(augmentations[:7]):\n",
    "            row = (idx + 1) // 4\n",
    "            col = (idx + 1) % 4\n",
    "            \n",
    "            # Apply transform\n",
    "            if isinstance(image, np.ndarray):\n",
    "                pil_image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "            else:\n",
    "                pil_image = image\n",
    "            \n",
    "            transformed = transform(pil_image)\n",
    "            \n",
    "            # Convert to displayable format\n",
    "            if isinstance(transformed, torch.Tensor):\n",
    "                if transformed.ndim == 3:\n",
    "                    transformed = transformed.squeeze().numpy()\n",
    "                else:\n",
    "                    transformed = transformed.numpy()\n",
    "            \n",
    "            axes[row, col].imshow(transformed, cmap='gray')\n",
    "            axes[row, col].set_title(name)\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create augmentation pipeline\n",
    "aug_pipeline = MedicalAugmentation()\n",
    "\n",
    "# Create sample image\n",
    "sample_image = np.random.rand(256, 256)\n",
    "\n",
    "# Visualize augmentations\n",
    "print(\"Medical Image Augmentation Examples:\")\n",
    "print(\"=\" * 60)\n",
    "aug_pipeline.show_augmentations(sample_image)\n",
    "print(\"\\n‚úÖ Augmentation pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 7: Introduction to MONAI Framework\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Introduction to MONAI (Medical Open Network for AI)\n",
    "- Use pre-built medical imaging transforms\n",
    "- Understand MONAI's advantages for medical imaging\n",
    "\n",
    "### üìñ Key Features\n",
    "**MONAI:** PyTorch-based framework specifically for medical imaging\n",
    "- Medical-specific transforms and augmentations\n",
    "- Pre-built networks optimized for medical tasks\n",
    "- Specialized loss functions (Dice, Focal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 MONAI basics (install first if needed)\n",
    "# !pip install monai\n",
    "\n",
    "try:\n",
    "    import monai\n",
    "    from monai.transforms import (\n",
    "        Compose, LoadImage, EnsureChannelFirst, ScaleIntensity,\n",
    "        RandRotate, RandFlip, Resize, ToTensor\n",
    "    )\n",
    "    from monai.networks.nets import UNet as MONAI_UNet\n",
    "    from monai.losses import DiceLoss\n",
    "    \n",
    "    print(f\"MONAI version: {monai.__version__}\")\n",
    "    print(\"‚úÖ MONAI loaded successfully!\\n\")\n",
    "    \n",
    "    # Define MONAI transforms\n",
    "    monai_transforms = Compose([\n",
    "        EnsureChannelFirst(),\n",
    "        ScaleIntensity(),\n",
    "        Resize((256, 256)),\n",
    "        RandRotate(range_x=15, prob=0.5),\n",
    "        RandFlip(spatial_axis=0, prob=0.5),\n",
    "    ])\n",
    "    \n",
    "    print(\"MONAI Transform Pipeline:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(monai_transforms)\n",
    "    print(\"\\n‚úÖ MONAI transforms defined!\")\n",
    "    \n",
    "    # Create MONAI U-Net\n",
    "    monai_unet = MONAI_UNet(\n",
    "        spatial_dims=2,\n",
    "        in_channels=1,\n",
    "        out_channels=2,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        strides=(2, 2, 2, 2),\n",
    "        num_res_units=2,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMONAI U-Net created!\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in monai_unet.parameters()):,}\")\n",
    "    \n",
    "    # MONAI Dice Loss\n",
    "    dice_loss = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "    print(\"\\n‚úÖ MONAI Dice Loss initialized!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è MONAI not installed. Install with: pip install monai\")\n",
    "    print(\"Continuing with PyTorch implementation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 8: Model Evaluation and Validation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Implement medical imaging metrics (Dice score, IoU)\n",
    "- Understand evaluation strategies for medical AI\n",
    "- Calculate sensitivity, specificity, and other clinical metrics\n",
    "\n",
    "### üìñ Key Metrics\n",
    "**Classification:**\n",
    "- Accuracy, Sensitivity (Recall), Specificity\n",
    "- Precision, F1-score\n",
    "- ROC-AUC, Precision-Recall AUC\n",
    "\n",
    "**Segmentation:**\n",
    "- Dice Score (F1 for segmentation)\n",
    "- IoU (Intersection over Union)\n",
    "- Hausdorff Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Implement segmentation metrics\n",
    "def dice_score(pred, target, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate Dice Score (F1 for segmentation)\n",
    "    \n",
    "    Args:\n",
    "        pred: Predicted segmentation (binary)\n",
    "        target: Ground truth segmentation (binary)\n",
    "        smooth: Smoothing factor to avoid division by zero\n",
    "    \n",
    "    Returns:\n",
    "        Dice score (0 to 1, higher is better)\n",
    "    \"\"\"\n",
    "    pred = pred.flatten()\n",
    "    target = target.flatten()\n",
    "    \n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "    \n",
    "    return dice\n",
    "\n",
    "def iou_score(pred, target, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU)\n",
    "    \n",
    "    Args:\n",
    "        pred: Predicted segmentation (binary)\n",
    "        target: Ground truth segmentation (binary)\n",
    "        smooth: Smoothing factor\n",
    "    \n",
    "    Returns:\n",
    "        IoU score (0 to 1, higher is better)\n",
    "    \"\"\"\n",
    "    pred = pred.flatten()\n",
    "    target = target.flatten()\n",
    "    \n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    return iou\n",
    "\n",
    "# Test with dummy data\n",
    "print(\"Segmentation Metrics Test:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create dummy prediction and target\n",
    "pred = torch.zeros(100, 100)\n",
    "pred[30:70, 30:70] = 1  # Predicted square\n",
    "\n",
    "target = torch.zeros(100, 100)\n",
    "target[25:75, 25:75] = 1  # Ground truth square (slightly larger)\n",
    "\n",
    "# Calculate metrics\n",
    "dice = dice_score(pred, target)\n",
    "iou = iou_score(pred, target)\n",
    "\n",
    "print(f\"Dice Score: {dice:.4f}\")\n",
    "print(f\"IoU Score: {iou:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].imshow(target, cmap='gray')\n",
    "axes[0].set_title('Ground Truth')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(pred, cmap='gray')\n",
    "axes[1].set_title('Prediction')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Overlap visualization\n",
    "overlap = np.zeros((100, 100, 3))\n",
    "overlap[:, :, 0] = target  # Red channel for ground truth\n",
    "overlap[:, :, 1] = pred     # Green channel for prediction\n",
    "axes[2].imshow(overlap)\n",
    "axes[2].set_title(f'Overlap (Dice={dice:.3f}, IoU={iou:.3f})')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Metrics calculated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Implement classification metrics\n",
    "def calculate_classification_metrics(y_true, y_pred, y_prob=None):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive classification metrics\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth labels\n",
    "        y_pred: Predicted labels\n",
    "        y_prob: Predicted probabilities (optional, for ROC-AUC)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    # Convert to numpy if torch tensors\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.cpu().numpy()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    # Calculate confusion matrix components\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Recall\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'precision': precision,\n",
    "        'f1_score': f1,\n",
    "        'true_positives': tp,\n",
    "        'true_negatives': tn,\n",
    "        'false_positives': fp,\n",
    "        'false_negatives': fn\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Test with dummy data\n",
    "print(\"Classification Metrics Test:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate dummy predictions\n",
    "np.random.seed(42)\n",
    "y_true = np.random.randint(0, 2, 100)\n",
    "y_pred = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_classification_metrics(y_true, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Accuracy:     {metrics['accuracy']:.4f}\")\n",
    "print(f\"Sensitivity:  {metrics['sensitivity']:.4f} (Recall)\")\n",
    "print(f\"Specificity:  {metrics['specificity']:.4f}\")\n",
    "print(f\"Precision:    {metrics['precision']:.4f}\")\n",
    "print(f\"F1 Score:     {metrics['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix Components:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"True Positives:  {metrics['true_positives']}\")\n",
    "print(f\"True Negatives:  {metrics['true_negatives']}\")\n",
    "print(f\"False Positives: {metrics['false_positives']}\")\n",
    "print(f\"False Negatives: {metrics['false_negatives']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Classification metrics calculated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Practice Complete!\n",
    "\n",
    "### Summary of What We Learned:\n",
    "\n",
    "1. **CNN Fundamentals**\n",
    "   - Manual convolution implementation\n",
    "   - Understanding kernels, stride, and padding\n",
    "\n",
    "2. **Model Architectures**\n",
    "   - Building CNN from scratch\n",
    "   - Transfer learning with pre-trained models\n",
    "   - U-Net for medical image segmentation\n",
    "\n",
    "3. **Interpretability**\n",
    "   - Grad-CAM for visualization\n",
    "   - Understanding model decisions\n",
    "\n",
    "4. **Data Augmentation**\n",
    "   - Medical-specific transformations\n",
    "   - Respecting anatomical constraints\n",
    "\n",
    "5. **MONAI Framework**\n",
    "   - Medical imaging specialized tools\n",
    "   - Pre-built networks and transforms\n",
    "\n",
    "6. **Model Evaluation**\n",
    "   - Segmentation metrics (Dice, IoU)\n",
    "   - Classification metrics (Sensitivity, Specificity)\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **Medical imaging requires specialized techniques** different from natural images\n",
    "- **Interpretability is crucial** for clinical acceptance and trust\n",
    "- **Proper evaluation metrics** are essential for medical AI validation\n",
    "- **Domain knowledge** should guide architecture and augmentation choices\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Implement a complete training pipeline** with real medical data\n",
    "2. **Explore 3D medical imaging** (CT, MRI volumes)\n",
    "3. **Study regulatory requirements** (FDA approval process)\n",
    "4. **Learn about multi-modal fusion** (combining different imaging modalities)\n",
    "5. **Practice with public datasets** (ChestX-ray14, LIDC-IDRI, BraTS)\n",
    "\n",
    "### Resources for Further Learning:\n",
    "\n",
    "- **MONAI Documentation:** https://monai.io/\n",
    "- **Medical Imaging Datasets:** https://grand-challenge.org/\n",
    "- **PyTorch Tutorials:** https://pytorch.org/tutorials/\n",
    "- **FDA Digital Health:** https://www.fda.gov/medical-devices/digital-health\n",
    "\n",
    "---\n",
    "\n",
    "## üè• Remember:\n",
    "\n",
    "**Medical AI is about improving patient care, not just achieving high accuracy metrics!**\n",
    "\n",
    "Always consider:\n",
    "- Clinical validity and utility\n",
    "- Patient safety and privacy\n",
    "- Regulatory compliance\n",
    "- Ethical implications\n",
    "- Bias and fairness across populations\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
