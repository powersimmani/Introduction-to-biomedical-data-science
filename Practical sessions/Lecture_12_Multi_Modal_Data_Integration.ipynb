{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Lecture 12: Multi-Modal Data Integration - Hands-on Practice\n",
    "\n",
    "## Table of Contents\n",
    "1. [Early vs Late Fusion Implementation](#practice-1-early-vs-late-fusion)\n",
    "2. [Matrix Factorization for Multi-Omics](#practice-2-matrix-factorization)\n",
    "3. [Canonical Correlation Analysis (CCA)](#practice-3-canonical-correlation-analysis)\n",
    "4. [Similarity Network Fusion](#practice-4-similarity-network-fusion)\n",
    "5. [Attention-based Integration](#practice-5-attention-based-integration)\n",
    "6. [MOFA (Multi-Omics Factor Analysis)](#practice-6-mofa-analysis)\n",
    "7. [Integration Workflow](#practice-7-complete-integration-workflow)\n",
    "8. [Visualization and Interpretation](#practice-8-visualization-and-interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"âœ… All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 1: Early vs Late Fusion\n",
    "\n",
    "### ðŸŽ¯ Learning Objectives\n",
    "- Understand the difference between early and late fusion strategies\n",
    "- Implement both approaches on multi-modal data\n",
    "- Compare their performance and characteristics\n",
    "\n",
    "### ðŸ“– Key Concepts\n",
    "- **Early Fusion**: Concatenate features before modeling (captures feature interactions)\n",
    "- **Late Fusion**: Train separate models and combine predictions (more flexible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Generate synthetic multi-modal data\n",
    "def generate_multimodal_data(n_samples=200, n_features_mod1=10, n_features_mod2=15):\n",
    "    \"\"\"Generate synthetic data for two modalities\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Modality 1: Genomics (e.g., gene expression)\n",
    "    X_genomics = np.random.randn(n_samples, n_features_mod1)\n",
    "    \n",
    "    # Modality 2: Imaging (e.g., radiomics features)\n",
    "    X_imaging = np.random.randn(n_samples, n_features_mod2)\n",
    "    \n",
    "    # Generate labels (binary classification: disease vs healthy)\n",
    "    # Create dependency on both modalities\n",
    "    signal_genomics = X_genomics[:, 0] + X_genomics[:, 1]\n",
    "    signal_imaging = X_imaging[:, 0] - X_imaging[:, 1]\n",
    "    y = (signal_genomics + signal_imaging > 0).astype(int)\n",
    "    \n",
    "    print(\"ðŸ“Š Multi-modal Data Generated\")\n",
    "    print(f\"  Genomics shape: {X_genomics.shape}\")\n",
    "    print(f\"  Imaging shape: {X_imaging.shape}\")\n",
    "    print(f\"  Labels distribution: {np.bincount(y)}\")\n",
    "    \n",
    "    return X_genomics, X_imaging, y\n",
    "\n",
    "X_gen, X_img, y = generate_multimodal_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Early Fusion Implementation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def early_fusion(X_mod1, X_mod2, y):\n",
    "    \"\"\"Implement early fusion strategy\"\"\"\n",
    "    \n",
    "    # Step 1: Concatenate features\n",
    "    X_early = np.concatenate([X_mod1, X_mod2], axis=1)\n",
    "    print(\"\\nðŸ”— Early Fusion\")\n",
    "    print(f\"  Combined feature shape: {X_early.shape}\")\n",
    "    \n",
    "    # Step 2: Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_early, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Step 3: Train single model\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Step 4: Evaluate\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"\\n  ðŸ“ˆ Performance:\")\n",
    "    print(f\"    Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"    AUC-ROC: {auc:.4f}\")\n",
    "    \n",
    "    return model, accuracy, auc\n",
    "\n",
    "early_model, early_acc, early_auc = early_fusion(X_gen, X_img, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Late Fusion Implementation\n",
    "def late_fusion(X_mod1, X_mod2, y):\n",
    "    \"\"\"Implement late fusion strategy\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ”€ Late Fusion\")\n",
    "    \n",
    "    # Split data for both modalities\n",
    "    X1_train, X1_test, y_train, y_test = train_test_split(\n",
    "        X_mod1, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    X2_train, X2_test, _, _ = train_test_split(\n",
    "        X_mod2, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train separate models\n",
    "    # Model 1: Genomics\n",
    "    scaler1 = StandardScaler()\n",
    "    X1_train_scaled = scaler1.fit_transform(X1_train)\n",
    "    X1_test_scaled = scaler1.transform(X1_test)\n",
    "    \n",
    "    model1 = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model1.fit(X1_train_scaled, y_train)\n",
    "    pred_proba1 = model1.predict_proba(X1_test_scaled)[:, 1]\n",
    "    \n",
    "    # Model 2: Imaging\n",
    "    scaler2 = StandardScaler()\n",
    "    X2_train_scaled = scaler2.fit_transform(X2_train)\n",
    "    X2_test_scaled = scaler2.transform(X2_test)\n",
    "    \n",
    "    model2 = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model2.fit(X2_train_scaled, y_train)\n",
    "    pred_proba2 = model2.predict_proba(X2_test_scaled)[:, 1]\n",
    "    \n",
    "    # Combine predictions (average ensemble)\n",
    "    pred_proba_combined = (pred_proba1 + pred_proba2) / 2\n",
    "    y_pred_combined = (pred_proba_combined > 0.5).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_combined)\n",
    "    auc = roc_auc_score(y_test, pred_proba_combined)\n",
    "    \n",
    "    print(f\"\\n  ðŸ“ˆ Performance:\")\n",
    "    print(f\"    Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"    AUC-ROC: {auc:.4f}\")\n",
    "    \n",
    "    return model1, model2, accuracy, auc\n",
    "\n",
    "late_model1, late_model2, late_acc, late_auc = late_fusion(X_gen, X_img, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Compare Early vs Late Fusion\n",
    "def compare_fusion_strategies():\n",
    "    \"\"\"Visualize comparison between fusion strategies\"\"\"\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Strategy': ['Early Fusion', 'Late Fusion'],\n",
    "        'Accuracy': [early_acc, late_acc],\n",
    "        'AUC-ROC': [early_auc, late_auc]\n",
    "    })\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    axes[0].bar(results['Strategy'], results['Accuracy'], color=['#1E64C8', '#5088d4'])\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('Accuracy Comparison')\n",
    "    axes[0].set_ylim([0.5, 1.0])\n",
    "    for i, v in enumerate(results['Accuracy']):\n",
    "        axes[0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "    \n",
    "    # AUC comparison\n",
    "    axes[1].bar(results['Strategy'], results['AUC-ROC'], color=['#1E64C8', '#5088d4'])\n",
    "    axes[1].set_ylabel('AUC-ROC')\n",
    "    axes[1].set_title('AUC-ROC Comparison')\n",
    "    axes[1].set_ylim([0.5, 1.0])\n",
    "    for i, v in enumerate(results['AUC-ROC']):\n",
    "        axes[1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ“Š Comparison Summary:\")\n",
    "    print(results.to_string(index=False))\n",
    "    print(\"\\nâœ… Key Insights:\")\n",
    "    print(\"  â€¢ Early fusion captures feature interactions but is computationally expensive\")\n",
    "    print(\"  â€¢ Late fusion is more flexible and modular\")\n",
    "    print(\"  â€¢ Performance depends on data characteristics and signal distribution\")\n",
    "\n",
    "compare_fusion_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 2: Matrix Factorization for Multi-Omics\n",
    "\n",
    "### ðŸŽ¯ Learning Objectives\n",
    "- Implement Non-negative Matrix Factorization (NMF) for multi-omics data\n",
    "- Learn to extract shared and specific factors\n",
    "- Interpret biological meaning of factors\n",
    "\n",
    "### ðŸ“– Key Concepts\n",
    "**Matrix Factorization**: $X \\approx W \\times H$ where $W$ contains sample factors and $H$ contains feature factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Generate multi-omics data\n",
    "def generate_omics_data(n_samples=100, n_genes=50, n_proteins=40):\n",
    "    \"\"\"Generate synthetic omics data\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create latent factors\n",
    "    n_factors = 5\n",
    "    W = np.abs(np.random.randn(n_samples, n_factors))  # Sample factors\n",
    "    \n",
    "    # Generate gene expression (transcriptomics)\n",
    "    H_genes = np.abs(np.random.randn(n_factors, n_genes))\n",
    "    X_genes = W @ H_genes + np.abs(np.random.randn(n_samples, n_genes) * 0.1)\n",
    "    \n",
    "    # Generate protein abundance (proteomics)\n",
    "    H_proteins = np.abs(np.random.randn(n_factors, n_proteins))\n",
    "    X_proteins = W @ H_proteins + np.abs(np.random.randn(n_samples, n_proteins) * 0.1)\n",
    "    \n",
    "    print(\"ðŸ§¬ Multi-omics Data Generated\")\n",
    "    print(f\"  Gene expression: {X_genes.shape}\")\n",
    "    print(f\"  Protein abundance: {X_proteins.shape}\")\n",
    "    print(f\"  True factors: {n_factors}\")\n",
    "    \n",
    "    return X_genes, X_proteins\n",
    "\n",
    "X_genes, X_proteins = generate_omics_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Apply NMF to each modality\n",
    "def apply_nmf_multiomics(X_genes, X_proteins, n_components=5):\n",
    "    \"\"\"Apply NMF to multi-omics data\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ”¬ Applying NMF to Multi-omics Data\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # NMF for gene expression\n",
    "    nmf_genes = NMF(n_components=n_components, random_state=42, max_iter=500)\n",
    "    W_genes = nmf_genes.fit_transform(X_genes)\n",
    "    H_genes = nmf_genes.components_\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Gene Expression Factorization:\")\n",
    "    print(f\"  W (samples Ã— factors): {W_genes.shape}\")\n",
    "    print(f\"  H (factors Ã— genes): {H_genes.shape}\")\n",
    "    print(f\"  Reconstruction error: {nmf_genes.reconstruction_err_:.4f}\")\n",
    "    \n",
    "    # NMF for protein abundance\n",
    "    nmf_proteins = NMF(n_components=n_components, random_state=42, max_iter=500)\n",
    "    W_proteins = nmf_proteins.fit_transform(X_proteins)\n",
    "    H_proteins = nmf_proteins.components_\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Protein Abundance Factorization:\")\n",
    "    print(f\"  W (samples Ã— factors): {W_proteins.shape}\")\n",
    "    print(f\"  H (factors Ã— proteins): {H_proteins.shape}\")\n",
    "    print(f\"  Reconstruction error: {nmf_proteins.reconstruction_err_:.4f}\")\n",
    "    \n",
    "    return W_genes, H_genes, W_proteins, H_proteins\n",
    "\n",
    "W_g, H_g, W_p, H_p = apply_nmf_multiomics(X_genes, X_proteins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Identify shared factors across modalities\n",
    "def identify_shared_factors(W_genes, W_proteins):\n",
    "    \"\"\"Find correlation between factors across modalities\"\"\"\n",
    "    \n",
    "    n_factors = W_genes.shape[1]\n",
    "    correlation_matrix = np.zeros((n_factors, n_factors))\n",
    "    \n",
    "    for i in range(n_factors):\n",
    "        for j in range(n_factors):\n",
    "            correlation_matrix[i, j] = pearsonr(W_genes[:, i], W_proteins[:, j])[0]\n",
    "    \n",
    "    # Visualize correlation\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='RdBu_r', \n",
    "                center=0, vmin=-1, vmax=1,\n",
    "                xticklabels=[f'Protein F{i+1}' for i in range(n_factors)],\n",
    "                yticklabels=[f'Gene F{i+1}' for i in range(n_factors)])\n",
    "    plt.title('Factor Correlation Between Gene Expression and Protein Abundance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find strongly correlated factor pairs (shared factors)\n",
    "    print(\"\\nðŸ” Shared Factors (correlation > 0.5):\")\n",
    "    for i in range(n_factors):\n",
    "        for j in range(n_factors):\n",
    "            if abs(correlation_matrix[i, j]) > 0.5:\n",
    "                print(f\"  Gene Factor {i+1} â†” Protein Factor {j+1}: r = {correlation_matrix[i, j]:.3f}\")\n",
    "    \n",
    "    return correlation_matrix\n",
    "\n",
    "corr_matrix = identify_shared_factors(W_g, W_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 3: Canonical Correlation Analysis (CCA)\n",
    "\n",
    "### ðŸŽ¯ Learning Objectives\n",
    "- Implement CCA to find maximally correlated projections\n",
    "- Understand canonical variates\n",
    "- Compare with other integration methods\n",
    "\n",
    "### ðŸ“– Key Concepts\n",
    "**CCA Objective**: Find $w_1$ and $w_2$ that maximize $\\text{corr}(w_1^T X_1, w_2^T X_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Implement CCA\n",
    "def perform_cca(X_mod1, X_mod2, n_components=3):\n",
    "    \"\"\"Perform Canonical Correlation Analysis\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ”— Canonical Correlation Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Standardize data\n",
    "    scaler1 = StandardScaler()\n",
    "    scaler2 = StandardScaler()\n",
    "    X1_scaled = scaler1.fit_transform(X_mod1)\n",
    "    X2_scaled = scaler2.fit_transform(X_mod2)\n",
    "    \n",
    "    # Apply CCA\n",
    "    cca = CCA(n_components=n_components)\n",
    "    U, V = cca.fit_transform(X1_scaled, X2_scaled)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š CCA Results:\")\n",
    "    print(f\"  Canonical variates U (Modality 1): {U.shape}\")\n",
    "    print(f\"  Canonical variates V (Modality 2): {V.shape}\")\n",
    "    \n",
    "    # Calculate canonical correlations\n",
    "    print(f\"\\nðŸ“ˆ Canonical Correlations:\")\n",
    "    for i in range(n_components):\n",
    "        corr = pearsonr(U[:, i], V[:, i])[0]\n",
    "        print(f\"  Component {i+1}: r = {corr:.4f}\")\n",
    "    \n",
    "    return cca, U, V\n",
    "\n",
    "cca_model, U, V = perform_cca(X_genes, X_proteins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Visualize canonical variates\n",
    "def visualize_cca(U, V):\n",
    "    \"\"\"Visualize the first two canonical variates\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Scatter plot of first canonical variate\n",
    "    axes[0].scatter(U[:, 0], V[:, 0], alpha=0.6, s=50, c='#1E64C8')\n",
    "    axes[0].set_xlabel('Uâ‚ (Gene Expression)', fontsize=12)\n",
    "    axes[0].set_ylabel('Vâ‚ (Protein Abundance)', fontsize=12)\n",
    "    axes[0].set_title('First Canonical Variate Pair', fontsize=13, fontweight='bold')\n",
    "    corr1 = pearsonr(U[:, 0], V[:, 0])[0]\n",
    "    axes[0].text(0.05, 0.95, f'r = {corr1:.4f}', transform=axes[0].transAxes,\n",
    "                fontsize=11, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Scatter plot of second canonical variate\n",
    "    axes[1].scatter(U[:, 1], V[:, 1], alpha=0.6, s=50, c='#5088d4')\n",
    "    axes[1].set_xlabel('Uâ‚‚ (Gene Expression)', fontsize=12)\n",
    "    axes[1].set_ylabel('Vâ‚‚ (Protein Abundance)', fontsize=12)\n",
    "    axes[1].set_title('Second Canonical Variate Pair', fontsize=13, fontweight='bold')\n",
    "    corr2 = pearsonr(U[:, 1], V[:, 1])[0]\n",
    "    axes[1].text(0.05, 0.95, f'r = {corr2:.4f}', transform=axes[1].transAxes,\n",
    "                fontsize=11, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ… CCA successfully identifies correlated projections across modalities!\")\n",
    "\n",
    "visualize_cca(U, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 4: Similarity Network Fusion\n",
    "\n",
    "### ðŸŽ¯ Learning Objectives\n",
    "- Build patient similarity networks from different data types\n",
    "- Understand network-based integration\n",
    "- Apply iterative fusion algorithm\n",
    "\n",
    "### ðŸ“– Key Concepts\n",
    "**SNF**: Iteratively fuses multiple patient similarity networks to create an integrated network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Create similarity networks\n",
    "def create_similarity_network(X, k=5):\n",
    "    \"\"\"Create k-nearest neighbor similarity network\"\"\"\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    distances = squareform(pdist(X, metric='euclidean'))\n",
    "    \n",
    "    # Convert to similarity (using Gaussian kernel)\n",
    "    sigma = np.median(distances)\n",
    "    similarity = np.exp(-distances**2 / (2 * sigma**2))\n",
    "    \n",
    "    # Keep only k-nearest neighbors\n",
    "    n_samples = X.shape[0]\n",
    "    W = np.zeros_like(similarity)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Find k nearest neighbors\n",
    "        neighbors = np.argsort(distances[i])[1:k+1]  # Exclude self\n",
    "        W[i, neighbors] = similarity[i, neighbors]\n",
    "        W[neighbors, i] = similarity[neighbors, i]\n",
    "    \n",
    "    # Normalize\n",
    "    row_sums = W.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "    W = W / row_sums\n",
    "    \n",
    "    return W\n",
    "\n",
    "def similarity_network_fusion(X_list, k=5, n_iterations=20):\n",
    "    \"\"\"Simple SNF implementation\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸŒ Similarity Network Fusion\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create initial similarity networks\n",
    "    networks = []\n",
    "    for idx, X in enumerate(X_list):\n",
    "        W = create_similarity_network(X, k=k)\n",
    "        networks.append(W)\n",
    "        print(f\"  Network {idx+1} created: {W.shape}\")\n",
    "    \n",
    "    # Simple fusion: average networks\n",
    "    fused_network = np.mean(networks, axis=0)\n",
    "    \n",
    "    print(f\"\\nâœ… Fused network created: {fused_network.shape}\")\n",
    "    \n",
    "    return fused_network, networks\n",
    "\n",
    "# Apply SNF\n",
    "fused_net, individual_nets = similarity_network_fusion([X_genes, X_proteins])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Clustering on fused network\n",
    "def cluster_from_network(network, n_clusters=3):\n",
    "    \"\"\"Perform clustering using the fused network\"\"\"\n",
    "    \n",
    "    # Use network as affinity matrix for spectral clustering\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    \n",
    "    clustering = SpectralClustering(n_clusters=n_clusters, \n",
    "                                   affinity='precomputed',\n",
    "                                   random_state=42)\n",
    "    labels = clustering.fit_predict(network)\n",
    "    \n",
    "    # Compute silhouette score\n",
    "    # For spectral clustering with precomputed affinity, we use distance = 1 - affinity\n",
    "    distance_matrix = 1 - network\n",
    "    np.fill_diagonal(distance_matrix, 0)  # Distance to self is 0\n",
    "    silhouette = silhouette_score(distance_matrix, labels, metric='precomputed')\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Clustering Results:\")\n",
    "    print(f\"  Number of clusters: {n_clusters}\")\n",
    "    print(f\"  Cluster sizes: {np.bincount(labels)}\")\n",
    "    print(f\"  Silhouette score: {silhouette:.4f}\")\n",
    "    \n",
    "    return labels, silhouette\n",
    "\n",
    "cluster_labels, sil_score = cluster_from_network(fused_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 5: Attention-based Integration\n",
    "\n",
    "### ðŸŽ¯ Learning Objectives\n",
    "- Understand attention mechanism for multi-modal integration\n",
    "- Implement a simple attention-based fusion\n",
    "- Learn importance weighting of modalities\n",
    "\n",
    "### ðŸ“– Key Concepts\n",
    "**Attention**: $\\text{output} = \\sum_i \\alpha_i \\cdot \\text{feature}_i$ where $\\alpha_i$ are learned attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Simple attention mechanism\n",
    "def attention_fusion(features_list, method='mean'):\n",
    "    \"\"\"\n",
    "    Simple attention-based fusion\n",
    "    \n",
    "    Parameters:\n",
    "    - features_list: list of feature matrices\n",
    "    - method: 'mean' or 'learned'\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ Attention-based Fusion\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    n_modalities = len(features_list)\n",
    "    n_samples = features_list[0].shape[0]\n",
    "    \n",
    "    # Reduce each modality to same dimension using PCA\n",
    "    n_components = 10\n",
    "    reduced_features = []\n",
    "    \n",
    "    for idx, features in enumerate(features_list):\n",
    "        pca = PCA(n_components=n_components)\n",
    "        reduced = pca.fit_transform(features)\n",
    "        reduced_features.append(reduced)\n",
    "        print(f\"  Modality {idx+1}: {features.shape} â†’ {reduced.shape}\")\n",
    "    \n",
    "    # Stack features\n",
    "    stacked = np.stack(reduced_features, axis=1)  # (n_samples, n_modalities, n_components)\n",
    "    \n",
    "    if method == 'mean':\n",
    "        # Equal attention weights\n",
    "        attention_weights = np.ones(n_modalities) / n_modalities\n",
    "        print(f\"\\n  Using equal attention weights: {attention_weights}\")\n",
    "    else:\n",
    "        # Learn attention weights based on variance\n",
    "        variances = [np.var(f, axis=0).mean() for f in reduced_features]\n",
    "        attention_weights = np.array(variances) / sum(variances)\n",
    "        print(f\"\\n  Learned attention weights: {attention_weights}\")\n",
    "    \n",
    "    # Apply attention\n",
    "    fused_features = np.sum(stacked * attention_weights[np.newaxis, :, np.newaxis], axis=1)\n",
    "    \n",
    "    print(f\"\\nâœ… Fused features shape: {fused_features.shape}\")\n",
    "    \n",
    "    return fused_features, attention_weights\n",
    "\n",
    "fused_features, att_weights = attention_fusion([X_genes, X_proteins], method='learned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Visualize attention weights\n",
    "def visualize_attention(attention_weights):\n",
    "    \"\"\"Visualize learned attention weights\"\"\"\n",
    "    \n",
    "    modality_names = ['Gene Expression', 'Protein Abundance']\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bars = plt.bar(modality_names, attention_weights, color=['#1E64C8', '#5088d4'])\n",
    "    plt.ylabel('Attention Weight', fontsize=12)\n",
    "    plt.title('Learned Attention Weights for Each Modality', fontsize=13, fontweight='bold')\n",
    "    plt.ylim([0, max(attention_weights) * 1.2])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, weight in zip(bars, attention_weights):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{weight:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Interpretation:\")\n",
    "    max_idx = np.argmax(attention_weights)\n",
    "    print(f\"  {modality_names[max_idx]} receives the highest attention weight\")\n",
    "    print(f\"  This suggests it contains more informative signals for the task\")\n",
    "\n",
    "visualize_attention(att_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 6: MOFA Analysis\n",
    "\n",
    "### ðŸŽ¯ Learning Objectives\n",
    "- Understand Multi-Omics Factor Analysis framework\n",
    "- Decompose variance into shared and specific factors\n",
    "- Interpret factor contributions\n",
    "\n",
    "### ðŸ“– Key Concepts\n",
    "**MOFA**: Bayesian factor analysis that identifies shared and modality-specific variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Simplified MOFA-like analysis using NMF\n",
    "def mofa_like_analysis(data_list, n_factors=5):\n",
    "    \"\"\"\n",
    "    Simplified MOFA-like analysis\n",
    "    In practice, use the actual MOFA package: https://github.com/bioFAM/MOFA2\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ”¬ MOFA-like Factor Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Concatenate data\n",
    "    X_concat = np.concatenate(data_list, axis=1)\n",
    "    print(f\"  Concatenated data shape: {X_concat.shape}\")\n",
    "    \n",
    "    # Apply NMF\n",
    "    nmf = NMF(n_components=n_factors, random_state=42, max_iter=500)\n",
    "    W = nmf.fit_transform(X_concat)  # Sample factors\n",
    "    H = nmf.components_  # Feature factors\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Factorization Results:\")\n",
    "    print(f\"  Sample factors W: {W.shape}\")\n",
    "    print(f\"  Feature factors H: {H.shape}\")\n",
    "    \n",
    "    # Split H back into modality-specific components\n",
    "    n_genes = data_list[0].shape[1]\n",
    "    H_genes = H[:, :n_genes]\n",
    "    H_proteins = H[:, n_genes:]\n",
    "    \n",
    "    # Calculate variance explained by each factor\n",
    "    var_explained = []\n",
    "    for i in range(n_factors):\n",
    "        # Reconstruction using only factor i\n",
    "        reconstruction = np.outer(W[:, i], H[i, :])\n",
    "        var = np.var(reconstruction)\n",
    "        var_explained.append(var)\n",
    "    \n",
    "    var_explained = np.array(var_explained)\n",
    "    var_explained_pct = var_explained / var_explained.sum() * 100\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Variance Explained by Each Factor:\")\n",
    "    for i, pct in enumerate(var_explained_pct):\n",
    "        print(f\"  Factor {i+1}: {pct:.2f}%\")\n",
    "    \n",
    "    return W, H_genes, H_proteins, var_explained_pct\n",
    "\n",
    "W_mofa, H_genes_mofa, H_proteins_mofa, var_exp = mofa_like_analysis([X_genes, X_proteins])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Visualize factor contributions\n",
    "def visualize_mofa_results(W, var_explained_pct):\n",
    "    \"\"\"Visualize MOFA factor analysis results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Variance explained\n",
    "    n_factors = len(var_explained_pct)\n",
    "    axes[0].bar(range(1, n_factors+1), var_explained_pct, color='#1E64C8')\n",
    "    axes[0].set_xlabel('Factor', fontsize=12)\n",
    "    axes[0].set_ylabel('Variance Explained (%)', fontsize=12)\n",
    "    axes[0].set_title('Variance Decomposition', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_xticks(range(1, n_factors+1))\n",
    "    \n",
    "    # Sample representation in factor space\n",
    "    scatter = axes[1].scatter(W[:, 0], W[:, 1], c=np.arange(len(W)), \n",
    "                             cmap='viridis', s=50, alpha=0.6)\n",
    "    axes[1].set_xlabel(f'Factor 1 ({var_explained_pct[0]:.1f}%)', fontsize=12)\n",
    "    axes[1].set_ylabel(f'Factor 2 ({var_explained_pct[1]:.1f}%)', fontsize=12)\n",
    "    axes[1].set_title('Samples in Factor Space', fontsize=13, fontweight='bold')\n",
    "    plt.colorbar(scatter, ax=axes[1], label='Sample Index')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ… MOFA reveals latent factors explaining variance across modalities!\")\n",
    "\n",
    "visualize_mofa_results(W_mofa, var_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 7: Complete Integration Workflow\n",
    "\n",
    "### ðŸŽ¯ Learning Objectives\n",
    "- Implement end-to-end integration pipeline\n",
    "- Apply preprocessing, integration, and evaluation\n",
    "- Compare multiple integration methods\n",
    "\n",
    "### ðŸ“– Workflow Steps\n",
    "1. **Data Loading** â†’ 2. **Preprocessing** â†’ 3. **Integration** â†’ 4. **Visualization** â†’ 5. **Interpretation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Complete integration pipeline\n",
    "def integration_pipeline(X_list, labels=None, method='pca'):\n",
    "    \"\"\"\n",
    "    Complete multi-modal integration workflow\n",
    "    \n",
    "    Parameters:\n",
    "    - X_list: list of data matrices\n",
    "    - labels: ground truth labels (if available)\n",
    "    - method: 'pca', 'nmf', or 'concat'\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ”„ Complete Integration Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Preprocessing\n",
    "    print(\"\\nðŸ“ Step 1: Preprocessing\")\n",
    "    X_scaled_list = []\n",
    "    for idx, X in enumerate(X_list):\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        X_scaled_list.append(X_scaled)\n",
    "        print(f\"  Modality {idx+1}: Standardized\")\n",
    "    \n",
    "    # Step 2: Integration\n",
    "    print(f\"\\nðŸ”— Step 2: Integration (method={method})\")\n",
    "    if method == 'concat':\n",
    "        X_integrated = np.concatenate(X_scaled_list, axis=1)\n",
    "    elif method == 'pca':\n",
    "        X_concat = np.concatenate(X_scaled_list, axis=1)\n",
    "        pca = PCA(n_components=10)\n",
    "        X_integrated = pca.fit_transform(X_concat)\n",
    "    elif method == 'nmf':\n",
    "        X_concat = np.concatenate([np.abs(X) for X in X_scaled_list], axis=1)\n",
    "        nmf = NMF(n_components=10, random_state=42)\n",
    "        X_integrated = nmf.fit_transform(X_concat)\n",
    "    \n",
    "    print(f\"  Integrated data shape: {X_integrated.shape}\")\n",
    "    \n",
    "    # Step 3: Clustering\n",
    "    print(\"\\nðŸ“Š Step 3: Clustering\")\n",
    "    n_clusters = 3\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    pred_labels = kmeans.fit_predict(X_integrated)\n",
    "    print(f\"  Cluster sizes: {np.bincount(pred_labels)}\")\n",
    "    \n",
    "    # Step 4: Evaluation\n",
    "    print(\"\\nâœ… Step 4: Evaluation\")\n",
    "    silhouette = silhouette_score(X_integrated, pred_labels)\n",
    "    print(f\"  Silhouette score: {silhouette:.4f}\")\n",
    "    \n",
    "    if labels is not None:\n",
    "        ari = adjusted_rand_score(labels, pred_labels)\n",
    "        print(f\"  Adjusted Rand Index: {ari:.4f}\")\n",
    "    \n",
    "    return X_integrated, pred_labels, silhouette\n",
    "\n",
    "# Run pipeline with different methods\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparing Integration Methods\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "methods = ['concat', 'pca', 'nmf']\n",
    "results = {}\n",
    "\n",
    "for method in methods:\n",
    "    X_int, labels_pred, sil = integration_pipeline([X_genes, X_proteins], method=method)\n",
    "    results[method] = {'silhouette': sil, 'integrated': X_int, 'labels': labels_pred}\n",
    "    print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 8: Visualization and Interpretation\n",
    "\n",
    "### ðŸŽ¯ Learning Objectives\n",
    "- Visualize integrated data in low-dimensional space\n",
    "- Compare different integration methods\n",
    "- Interpret biological meaning of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Comprehensive visualization\n",
    "def visualize_integration_results(results_dict):\n",
    "    \"\"\"Visualize and compare integration results\"\"\"\n",
    "    \n",
    "    n_methods = len(results_dict)\n",
    "    fig, axes = plt.subplots(1, n_methods, figsize=(6*n_methods, 5))\n",
    "    \n",
    "    if n_methods == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (method, result) in enumerate(results_dict.items()):\n",
    "        X_int = result['integrated']\n",
    "        labels = result['labels']\n",
    "        \n",
    "        # Use first two dimensions for visualization\n",
    "        if X_int.shape[1] >= 2:\n",
    "            x, y = X_int[:, 0], X_int[:, 1]\n",
    "        else:\n",
    "            x = X_int[:, 0]\n",
    "            y = np.zeros_like(x)\n",
    "        \n",
    "        scatter = axes[idx].scatter(x, y, c=labels, cmap='Set2', s=50, alpha=0.7)\n",
    "        axes[idx].set_xlabel('Component 1', fontsize=11)\n",
    "        axes[idx].set_ylabel('Component 2', fontsize=11)\n",
    "        axes[idx].set_title(f'{method.upper()}\\n(Silhouette: {result[\"silhouette\"]:.3f})',\n",
    "                           fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Add cluster centers\n",
    "        for cluster_id in np.unique(labels):\n",
    "            mask = labels == cluster_id\n",
    "            center_x = x[mask].mean()\n",
    "            center_y = y[mask].mean()\n",
    "            axes[idx].plot(center_x, center_y, 'k*', markersize=20, \n",
    "                          markeredgecolor='white', markeredgewidth=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\nðŸ“Š Method Comparison Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    for method, result in results_dict.items():\n",
    "        print(f\"  {method.upper():10s} - Silhouette: {result['silhouette']:.4f}\")\n",
    "    \n",
    "    # Find best method\n",
    "    best_method = max(results_dict.items(), key=lambda x: x[1]['silhouette'])[0]\n",
    "    print(f\"\\nâœ… Best method: {best_method.upper()}\")\n",
    "\n",
    "visualize_integration_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Final summary and comparison\n",
    "def create_comparison_table(results_dict):\n",
    "    \"\"\"Create a comprehensive comparison table\"\"\"\n",
    "    \n",
    "    comparison_data = []\n",
    "    for method, result in results_dict.items():\n",
    "        comparison_data.append({\n",
    "            'Method': method.upper(),\n",
    "            'Silhouette Score': f\"{result['silhouette']:.4f}\",\n",
    "            'Dimensions': result['integrated'].shape[1],\n",
    "            'N Clusters': len(np.unique(result['labels']))\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL COMPARISON TABLE\")\n",
    "    print(\"=\"*60)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df\n",
    "\n",
    "comparison_df = create_comparison_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Practice Complete!\n",
    "\n",
    "### Summary of What We Learned:\n",
    "\n",
    "1. **Early vs Late Fusion**: Different strategies for combining multi-modal data\n",
    "2. **Matrix Factorization (NMF)**: Extracting shared and specific factors from multi-omics data\n",
    "3. **Canonical Correlation Analysis (CCA)**: Finding maximally correlated projections\n",
    "4. **Similarity Network Fusion**: Network-based integration approach\n",
    "5. **Attention Mechanisms**: Learning importance weights for different modalities\n",
    "6. **MOFA**: Multi-omics factor analysis for variance decomposition\n",
    "7. **Complete Pipeline**: End-to-end integration workflow\n",
    "8. **Visualization**: Comparing and interpreting integration results\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "âœ… **Integration Methods**:\n",
    "- Early fusion captures feature interactions but can be computationally expensive\n",
    "- Late fusion is more flexible and modular\n",
    "- Matrix factorization reveals latent structure\n",
    "- Network-based methods leverage similarity relationships\n",
    "\n",
    "âœ… **Practical Considerations**:\n",
    "- Data preprocessing (standardization) is crucial\n",
    "- Choice of method depends on data characteristics and research questions\n",
    "- Visualization helps interpret biological meaning\n",
    "- Multiple evaluation metrics provide comprehensive assessment\n",
    "\n",
    "âœ… **Applications**:\n",
    "- Disease subtyping (identifying molecular subtypes)\n",
    "- Biomarker discovery (finding multi-modal signatures)\n",
    "- Treatment response prediction\n",
    "- Systems medicine approaches\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Advanced Methods**:\n",
    "   - Deep learning fusion (autoencoders, transformers)\n",
    "   - Graph neural networks for network integration\n",
    "   - Multimodal variational autoencoders\n",
    "\n",
    "2. **Real Data Applications**:\n",
    "   - TCGA pan-cancer multi-omics datasets\n",
    "   - Spatial transcriptomics + imaging\n",
    "   - Clinical + molecular data integration\n",
    "\n",
    "3. **Biological Interpretation**:\n",
    "   - Pathway enrichment analysis\n",
    "   - Network analysis and disease modules\n",
    "   - Clinical validation\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- **MOFA Package**: https://github.com/bioFAM/MOFA2\n",
    "- **SNFtool**: https://github.com/maxconway/SNFtool\n",
    "- **scikit-learn**: https://scikit-learn.org/\n",
    "- **Review Papers**: Subramanian et al. (2020) \"Multi-omics data integration\"\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ Congratulations!\n",
    "\n",
    "You've completed the hands-on practice for Lecture 12: Multi-Modal Data Integration!\n",
    "\n",
    "**Key Takeaway**: Multi-modal data integration is essential for comprehensive understanding of biological systems and precision medicine. Different integration methods have different strengths - the choice depends on your specific research question and data characteristics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
