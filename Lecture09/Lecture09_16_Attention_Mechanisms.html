<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Attention Mechanisms</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: Aptos, 'Segoe UI', sans-serif;
            background: white;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            padding: 40px 20px;
        }
        
        .container {
            width: 960px;
            padding: 35px 45px;
            background: white;
        }
        
        .title {
            font-size: 28px;
            font-weight: 600;
            color: #1E64C8;
            margin-bottom: 30px;
            text-align: center;
        }
        
        .concept-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 20px;
        }
        
        .concept-card {
            background: white;
            border: 2.5px solid #1E64C8;
            border-radius: 10px;
            padding: 18px;
            transition: all 0.3s;
            min-height: 140px;
        }
        
        .concept-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 6px 20px rgba(30, 100, 200, 0.2);
            background: #f8fbff;
        }
        
        .concept-name {
            font-size: 18px;
            font-weight: 700;
            color: #1E64C8;
            margin-bottom: 10px;
        }
        
        .concept-desc {
            font-size: 15px;
            color: #333;
            line-height: 1.5;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="title">Attention Mechanisms</div>
        
        <div class="concept-grid">

            <div class="concept-card">
                <div class="concept-name">Self-Attention</div>
                <div class="concept-desc">Capture long-range dependencies. Every position attends to all others</div>
            </div>
        
            <div class="concept-card">
                <div class="concept-name">Cross-Attention</div>
                <div class="concept-desc">Attend between different modalities or sequences. Query from one, key/value from another</div>
            </div>
        
            <div class="concept-card">
                <div class="concept-name">Vision Transformers</div>
                <div class="concept-desc">Pure attention-based architecture. ViT, Swin Transformer for medical imaging</div>
            </div>
        
            <div class="concept-card">
                <div class="concept-name">Hybrid Architectures</div>
                <div class="concept-desc">CNN backbone + Transformer head. CoAtNet, TransUNet combine local and global context</div>
            </div>
        
            <div class="concept-card">
                <div class="concept-name">Interpretability Benefits</div>
                <div class="concept-desc">Attention maps show model focus. More intuitive than CNN activation maps</div>
            </div>
        
        </div>
    </div>
</body>
</html>